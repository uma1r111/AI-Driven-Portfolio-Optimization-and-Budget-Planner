{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "rzYJKYVjJIyl",
        "outputId": "fabbe6da-bede-4cc2-d9fe-6a3f3da20cf7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-905371f5-5358-494f-a973-73fe1b3b714b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-905371f5-5358-494f-a973-73fe1b3b714b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving combined_stock_features.csv to combined_stock_features.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Click \"Choose Files\" and select your products.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvpFyqFiL3Z0",
        "outputId": "a1258afe-4645-44ad-ac3a-9a0ac53bdab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Date       Close        High         Low        Open  \\\n",
            "0      2023-01-03  123.632523  129.395510  122.742865  128.782641   \n",
            "1      2023-01-04  124.907707  127.181276  123.642420  125.431615   \n",
            "2      2023-01-05  123.583107  126.301500  123.326101  125.668857   \n",
            "3      2023-01-06  128.130234  128.792531  123.454601  124.561732   \n",
            "4      2023-01-09  128.654129  131.876670  128.397123  128.970458   \n",
            "...           ...         ...         ...         ...         ...   \n",
            "24495  2023-12-22   97.674698   98.661891   97.588434   98.058068   \n",
            "24496  2023-12-26   97.895142   98.748153   97.875976   98.470205   \n",
            "24497  2023-12-27   97.435097   98.288108   97.128389   97.799302   \n",
            "24498  2023-12-28   96.026192   97.387176   95.968680   97.176318   \n",
            "24499  2023-12-29   95.824921   96.409563   95.479881   96.198712   \n",
            "\n",
            "            Volume        MA10        MA50       MA100       MA200        RSI  \\\n",
            "0      112117500.0  128.989245  141.020710  146.761524  149.348864  22.617091   \n",
            "1       89113600.0  128.395155  140.612138  146.347824  149.166576  23.287722   \n",
            "2       80962700.0  127.675524  140.134046  145.885253  148.970777  23.902488   \n",
            "3       87754700.0  127.099226  139.689857  145.457396  148.780787  39.493006   \n",
            "4       70790800.0  126.893618  139.315160  145.036356  148.586578  43.030698   \n",
            "...            ...         ...         ...         ...         ...        ...   \n",
            "24495   12921800.0   96.670255   99.728154  103.023651  101.943890  48.316031   \n",
            "24496   16835100.0   96.911782   99.599063  102.993496  101.930071  56.213457   \n",
            "24497   14558800.0   97.257780   99.459252  102.955915  101.919742  59.937696   \n",
            "24498   16329300.0   97.379502   99.263909  102.906317  101.900501  56.502621   \n",
            "24499   17741400.0   97.226153   99.034909  102.849713  101.905096  51.694294   \n",
            "\n",
            "       Norm_Volume  Bollinger_Upper  Bollinger_Lower  Log_Returns  Pct_Change  \\\n",
            "0         1.331549       148.054165       121.250639    -0.038122   -0.037405   \n",
            "1         1.057702       146.806196       120.494905     0.010261    0.010314   \n",
            "2         0.959823       146.141773       119.390892    -0.010661   -0.010605   \n",
            "3         1.037141       145.362317       119.051357     0.036133    0.036794   \n",
            "4         0.840106       144.150868       119.027170     0.004080    0.004089   \n",
            "...            ...              ...              ...          ...         ...   \n",
            "24495     0.588664       100.445159        93.591178     0.001768    0.001769   \n",
            "24496     0.776447       100.160120        93.701780     0.002254    0.002257   \n",
            "24497     0.673882        99.815750        93.831460    -0.004710   -0.004699   \n",
            "24498     0.758253        99.671194        93.769953    -0.014565   -0.014460   \n",
            "24499     0.828160        99.444343        93.732276    -0.002098   -0.002096   \n",
            "\n",
            "      Ticker                       Sector  \n",
            "0       AAPL                   Technology  \n",
            "1       AAPL                   Technology  \n",
            "2       AAPL                   Technology  \n",
            "3       AAPL                   Technology  \n",
            "4       AAPL                   Technology  \n",
            "...      ...                          ...  \n",
            "24495    XOM  Physical Assets & Resources  \n",
            "24496    XOM  Physical Assets & Resources  \n",
            "24497    XOM  Physical Assets & Resources  \n",
            "24498    XOM  Physical Assets & Resources  \n",
            "24499    XOM  Physical Assets & Resources  \n",
            "\n",
            "[24500 rows x 18 columns]\n",
            "             Date       Close        High         Low        Open  \\\n",
            "0      2023-01-03  123.632523  129.395510  122.742865  128.782641   \n",
            "1      2023-01-04  124.907707  127.181276  123.642420  125.431615   \n",
            "2      2023-01-05  123.583107  126.301500  123.326101  125.668857   \n",
            "3      2023-01-06  128.130234  128.792531  123.454601  124.561732   \n",
            "4      2023-01-09  128.654129  131.876670  128.397123  128.970458   \n",
            "...           ...         ...         ...         ...         ...   \n",
            "24495  2023-12-22   97.674698   98.661891   97.588434   98.058068   \n",
            "24496  2023-12-26   97.895142   98.748153   97.875976   98.470205   \n",
            "24497  2023-12-27   97.435097   98.288108   97.128389   97.799302   \n",
            "24498  2023-12-28   96.026192   97.387176   95.968680   97.176318   \n",
            "24499  2023-12-29   95.824921   96.409563   95.479881   96.198712   \n",
            "\n",
            "            Volume        MA10        MA50       MA100       MA200        RSI  \\\n",
            "0      112117500.0  128.989245  141.020710  146.761524  149.348864  22.617091   \n",
            "1       89113600.0  128.395155  140.612138  146.347824  149.166576  23.287722   \n",
            "2       80962700.0  127.675524  140.134046  145.885253  148.970777  23.902488   \n",
            "3       87754700.0  127.099226  139.689857  145.457396  148.780787  39.493006   \n",
            "4       70790800.0  126.893618  139.315160  145.036356  148.586578  43.030698   \n",
            "...            ...         ...         ...         ...         ...        ...   \n",
            "24495   12921800.0   96.670255   99.728154  103.023651  101.943890  48.316031   \n",
            "24496   16835100.0   96.911782   99.599063  102.993496  101.930071  56.213457   \n",
            "24497   14558800.0   97.257780   99.459252  102.955915  101.919742  59.937696   \n",
            "24498   16329300.0   97.379502   99.263909  102.906317  101.900501  56.502621   \n",
            "24499   17741400.0   97.226153   99.034909  102.849713  101.905096  51.694294   \n",
            "\n",
            "       Norm_Volume  Bollinger_Upper  Bollinger_Lower  Log_Returns  Pct_Change  \\\n",
            "0         1.331549       148.054165       121.250639    -0.038122   -0.037405   \n",
            "1         1.057702       146.806196       120.494905     0.010261    0.010314   \n",
            "2         0.959823       146.141773       119.390892    -0.010661   -0.010605   \n",
            "3         1.037141       145.362317       119.051357     0.036133    0.036794   \n",
            "4         0.840106       144.150868       119.027170     0.004080    0.004089   \n",
            "...            ...              ...              ...          ...         ...   \n",
            "24495     0.588664       100.445159        93.591178     0.001768    0.001769   \n",
            "24496     0.776447       100.160120        93.701780     0.002254    0.002257   \n",
            "24497     0.673882        99.815750        93.831460    -0.004710   -0.004699   \n",
            "24498     0.758253        99.671194        93.769953    -0.014565   -0.014460   \n",
            "24499     0.828160        99.444343        93.732276    -0.002098   -0.002096   \n",
            "\n",
            "      Ticker                       Sector  \n",
            "0       AAPL                   Technology  \n",
            "1       AAPL                   Technology  \n",
            "2       AAPL                   Technology  \n",
            "3       AAPL                   Technology  \n",
            "4       AAPL                   Technology  \n",
            "...      ...                          ...  \n",
            "24495    XOM  Physical Assets & Resources  \n",
            "24496    XOM  Physical Assets & Resources  \n",
            "24497    XOM  Physical Assets & Resources  \n",
            "24498    XOM  Physical Assets & Resources  \n",
            "24499    XOM  Physical Assets & Resources  \n",
            "\n",
            "[24500 rows x 18 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Read the uploaded file\n",
        "df = pd.read_csv(io.BytesIO(uploaded['combined_stock_features.csv']))\n",
        "print(df)\n",
        "\n",
        "# Or if you just want to save it first:\n",
        "with open('combined_stock_features.csv', 'wb') as f:\n",
        "    f.write(uploaded['combined_stock_features.csv'])\n",
        "\n",
        "df = pd.read_csv('combined_stock_features.csv')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "58rbigxyL6Tm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXzxPWQqMCZi"
      },
      "outputs": [],
      "source": [
        "# # Load the enhanced dataset\n",
        "# def load_enhanced_data(ticker=\"MSFT\"):\n",
        "#     df = pd.read_csv(f\"{ticker}_stock_features.csv\")\n",
        "#     df['Date'] = pd.to_datetime(df['Date'])  # Ensure Date is datetime\n",
        "#     df.set_index('Date', inplace=True)\n",
        "#     return df\n",
        "\n",
        "# ticker = \"MSFT\"\n",
        "# data = load_enhanced_data(ticker)\n",
        "# print(data.head())  # Verify all features are loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "874NpZVLpug4"
      },
      "outputs": [],
      "source": [
        "# Create a mapping for Ticker and Sector\n",
        "ticker_to_id = {ticker: idx for idx, ticker in enumerate(df['Ticker'].unique())}\n",
        "sector_to_id = {sector: idx for idx, sector in enumerate(df['Sector'].unique())}\n",
        "\n",
        "# Add Ticker_ID and Sector_ID columns\n",
        "df['Ticker_ID'] = df['Ticker'].map(ticker_to_id)\n",
        "df['Sector_ID'] = df['Sector'].map(sector_to_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ticker_to_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFF3QpeVm7wO",
        "outputId": "d32ed204-e728-489a-eadf-d07274999421"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AAPL': 0,\n",
              " 'ABBV': 1,\n",
              " 'ABT': 2,\n",
              " 'ACN': 3,\n",
              " 'ADBE': 4,\n",
              " 'AIG': 5,\n",
              " 'AMGN': 6,\n",
              " 'AMT': 7,\n",
              " 'AMZN': 8,\n",
              " 'AVGO': 9,\n",
              " 'AXP': 10,\n",
              " 'BA': 11,\n",
              " 'BAC': 12,\n",
              " 'BK': 13,\n",
              " 'BKNG': 14,\n",
              " 'BLK': 15,\n",
              " 'BMY': 16,\n",
              " 'C': 17,\n",
              " 'CAT': 18,\n",
              " 'CHTR': 19,\n",
              " 'CL': 20,\n",
              " 'CMCSA': 21,\n",
              " 'COF': 22,\n",
              " 'COP': 23,\n",
              " 'COST': 24,\n",
              " 'CRM': 25,\n",
              " 'CSCO': 26,\n",
              " 'CVS': 27,\n",
              " 'CVX': 28,\n",
              " 'DHR': 29,\n",
              " 'DIS': 30,\n",
              " 'DOW': 31,\n",
              " 'DUK': 32,\n",
              " 'EMR': 33,\n",
              " 'EXC': 34,\n",
              " 'F': 35,\n",
              " 'FDX': 36,\n",
              " 'GD': 37,\n",
              " 'GE': 38,\n",
              " 'GILD': 39,\n",
              " 'GM': 40,\n",
              " 'GOOG': 41,\n",
              " 'GS': 42,\n",
              " 'HD': 43,\n",
              " 'HON': 44,\n",
              " 'IBM': 45,\n",
              " 'INTC': 46,\n",
              " 'INTU': 47,\n",
              " 'JNJ': 48,\n",
              " 'JPM': 49,\n",
              " 'KHC': 50,\n",
              " 'KO': 51,\n",
              " 'LIN': 52,\n",
              " 'LLY': 53,\n",
              " 'LMT': 54,\n",
              " 'LOW': 55,\n",
              " 'MA': 56,\n",
              " 'MCD': 57,\n",
              " 'MDLZ': 58,\n",
              " 'MDT': 59,\n",
              " 'MET': 60,\n",
              " 'META': 61,\n",
              " 'MMM': 62,\n",
              " 'MO': 63,\n",
              " 'MRK': 64,\n",
              " 'MS': 65,\n",
              " 'MSFT': 66,\n",
              " 'NEE': 67,\n",
              " 'NFLX': 68,\n",
              " 'NKE': 69,\n",
              " 'NVDA': 70,\n",
              " 'ORCL': 71,\n",
              " 'PEP': 72,\n",
              " 'PFE': 73,\n",
              " 'PG': 74,\n",
              " 'PM': 75,\n",
              " 'PYPL': 76,\n",
              " 'QCOM': 77,\n",
              " 'RTX': 78,\n",
              " 'SBUX': 79,\n",
              " 'SO': 80,\n",
              " 'SPG': 81,\n",
              " 'T': 82,\n",
              " 'TGT': 83,\n",
              " 'TMO': 84,\n",
              " 'TMUS': 85,\n",
              " 'TSLA': 86,\n",
              " 'TXN': 87,\n",
              " 'UNH': 88,\n",
              " 'UNP': 89,\n",
              " 'UPS': 90,\n",
              " 'USB': 91,\n",
              " 'V': 92,\n",
              " 'VZ': 93,\n",
              " 'WBA': 94,\n",
              " 'WFC': 95,\n",
              " 'WMT': 96,\n",
              " 'XOM': 97}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sector_to_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d23RYalFsddB",
        "outputId": "1d11cd98-afcf-4d18-a48f-ebbb2aae6177"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Technology': 0,\n",
              " 'Healthcare': 1,\n",
              " 'Financial Services': 2,\n",
              " 'Physical Assets & Resources': 3,\n",
              " 'Consumer Cyclical': 4,\n",
              " 'Industrials': 5,\n",
              " 'Communication Services': 6,\n",
              " 'Consumer Defensive': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "oi8WkXezMHFR"
      },
      "outputs": [],
      "source": [
        "# Select features and target\n",
        "features = ['Close', 'MA100', 'RSI', 'Norm_Volume', 'Bollinger_Upper', 'Bollinger_Lower', 'Log_Returns', 'Ticker_ID', 'Sector_ID']\n",
        "data = df[features]  # Keep only selected features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQDJpNhJjMW6",
        "outputId": "bac7f053-09da-4430-862b-3e751310406d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "data['Sector_ID'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "7B1D-B29MIKN"
      },
      "outputs": [],
      "source": [
        "def min_max_scaling(data):\n",
        "    \"\"\"Scale data to [0, 1] range and return scaled data + min/max values\"\"\"\n",
        "    min_val = np.min(data)\n",
        "    max_val = np.max(data)\n",
        "    # Handle case where all values are identical (avoid division by zero)\n",
        "    if max_val == min_val:\n",
        "        return np.zeros_like(data), min_val, max_val\n",
        "    return (data - min_val) / (max_val - min_val), min_val, max_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "R1fDmlHWMJvB"
      },
      "outputs": [],
      "source": [
        "def scale_features(df):\n",
        "    scaled_data = {}\n",
        "    scalers = {}\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col in ['Ticker_ID', 'Sector_ID']:  # Don't scale these columns\n",
        "            scaled_data[col] = df[col].values\n",
        "            continue\n",
        "\n",
        "        scaled_values, min_val, max_val = min_max_scaling(df[col].values)\n",
        "        if col == 'Log_Returns':\n",
        "            scalers['target'] = (min_val, max_val)\n",
        "        else:\n",
        "            scalers[col] = (min_val, max_val)\n",
        "\n",
        "        scaled_data[col] = scaled_values\n",
        "\n",
        "    return pd.DataFrame(scaled_data), scalers\n",
        "\n",
        "# Scale all features\n",
        "scaled_data, scalers = scale_features(data)\n",
        "\n",
        "# Split into train/test (keeping temporal order)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(scaled_data) * split_ratio)\n",
        "train_data = scaled_data.iloc[:split_index]\n",
        "test_data = scaled_data.iloc[split_index:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGHXXjsMzRN1",
        "outputId": "f81cd291-ebc5-4791-d64e-c33833cb699e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Close', 'MA100', 'RSI', 'Norm_Volume', 'Bollinger_Upper',\n",
            "       'Bollinger_Lower', 'Log_Returns', 'Ticker_ID', 'Sector_ID'],\n",
            "      dtype='object')\n",
            "Index(['Close', 'MA100', 'RSI', 'Norm_Volume', 'Bollinger_Upper',\n",
            "       'Bollinger_Lower', 'Log_Returns', 'Ticker_ID', 'Sector_ID'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(train_data.columns)\n",
        "print(test_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "eaeTYKXqxYXV"
      },
      "outputs": [],
      "source": [
        "# Create ticker mapping and ticker IDs\n",
        "unique_tickers = list(set(train_data['Ticker_ID'].values) | set(test_data['Ticker_ID'].values))\n",
        "ticker_mapping = {ticker: idx for idx, ticker in enumerate(unique_tickers)}\n",
        "\n",
        "unique_sectors = list(set(train_data['Sector_ID'].values) | set(test_data['Sector_ID'].values))\n",
        "sector_mapping = {sector_id: sector_id for sector_id in unique_sectors}\n",
        "\n",
        "train_ticker_ids = train_data['Ticker_ID'].values\n",
        "test_ticker_ids = test_data['Ticker_ID'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mymmI-pfMLbm",
        "outputId": "0d1c66ed-4fe7-4500-926f-6fa48e50a62a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training shapes - X: (15650, 50, 6), y: (15650,)\n",
            "Test shapes - X: (3900, 50, 6), y: (3900,)\n"
          ]
        }
      ],
      "source": [
        "# Create sequences function for sector-wise training\n",
        "def create_sequences_multi(data, ticker_ids, sector_ids, seq_length=50, target_col='Log_Returns'):\n",
        "    \"\"\"Create sequences from multiple features (without including Ticker_ID) for sector-wise training\"\"\"\n",
        "    X, y, ticker_seq_list, sector_seq_list = [], [], [], []\n",
        "    data_values = data.drop(['Ticker_ID', 'Sector_ID'], axis=1).values  # Drop 'Ticker_ID' and 'Sector_ID' from features\n",
        "\n",
        "    for sector_id in np.unique(sector_ids):\n",
        "        sector_data = data[data['Sector_ID'] == sector_id]\n",
        "        sector_ticker_ids = sector_data['Ticker_ID'].values\n",
        "\n",
        "        for ticker_id in np.unique(sector_ticker_ids):\n",
        "            ticker_data = sector_data[sector_data['Ticker_ID'] == ticker_id]\n",
        "            features = ticker_data.drop(['Log_Returns', 'Ticker_ID', 'Sector_ID'], axis=1).values\n",
        "            targets = ticker_data['Log_Returns'].values\n",
        "\n",
        "            for i in range(len(features) - seq_length):\n",
        "                X.append(features[i:i+seq_length, :])  # Selecting only the features\n",
        "                ticker_seq_list.append(ticker_id)\n",
        "                sector_seq_list.append(sector_id)  # Include the sector_id in the sequence\n",
        "                y.append(targets[i + seq_length])\n",
        "\n",
        "    return np.array(X), np.array(y), np.array(ticker_seq_list), np.array(sector_seq_list)\n",
        "\n",
        "\n",
        "# Create sequences for train and test data sector-wise\n",
        "seq_length=50\n",
        "X_train, y_train, train_tickers, train_sectors = create_sequences_multi(train_data, train_ticker_ids, train_data['Sector_ID'].values, seq_length=50)\n",
        "X_test, y_test, test_tickers, test_sectors = create_sequences_multi(test_data, test_ticker_ids, test_data['Sector_ID'].values, seq_length=50)\n",
        "\n",
        "# Check shapes\n",
        "print(f\"Training shapes - X: {X_train.shape}, y: {y_train.shape}\")\n",
        "print(f\"Test shapes - X: {X_test.shape}, y: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "ROZ8S4AnMNKO"
      },
      "outputs": [],
      "source": [
        "class LSTM:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, ticker_dim, embedding_dim, sector_dim, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.lr = learning_rate\n",
        "        self.ticker_dim = ticker_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sector_dim = sector_dim  # Number of sectors\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.t = 1  # Time step for Adam\n",
        "\n",
        "        # Combined input size: hidden + input + ticker + sector embedding\n",
        "        concat_dim = hidden_dim + input_dim + embedding_dim + sector_dim\n",
        "\n",
        "        # Initialize weights\n",
        "        self.Wf = np.random.randn(hidden_dim, concat_dim) * 0.01\n",
        "        self.bf = np.zeros((hidden_dim, 1))\n",
        "\n",
        "        self.Wi = np.random.randn(hidden_dim, concat_dim) * 0.01\n",
        "        self.bi = np.zeros((hidden_dim, 1))\n",
        "\n",
        "        self.Wc = np.random.randn(hidden_dim, concat_dim) * 0.01\n",
        "        self.bc = np.zeros((hidden_dim, 1))\n",
        "\n",
        "        self.Wo = np.random.randn(hidden_dim, concat_dim) * 0.01\n",
        "        self.bo = np.zeros((hidden_dim, 1))\n",
        "\n",
        "        # Output layer\n",
        "        self.Wy = np.random.randn(output_dim, hidden_dim) * 0.01\n",
        "        self.by = np.zeros((output_dim, 1))\n",
        "\n",
        "        # Initialize embedding matrices\n",
        "        self.ticker_embedding = np.random.randn(self.ticker_dim, self.embedding_dim) * 0.01\n",
        "        self.sector_embedding = np.random.randn(self.sector_dim, self.embedding_dim) * 0.01\n",
        "\n",
        "        # Initialize Adam moment estimates\n",
        "        self._init_adam_params()\n",
        "\n",
        "    def _init_adam_params(self):\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        for param_name in ['Wf', 'Wi', 'Wc', 'Wo', 'Wy', 'bf', 'bi', 'bc', 'bo', 'by']:\n",
        "            param = getattr(self, param_name)\n",
        "            self.m[param_name] = np.zeros_like(param)\n",
        "            self.v[param_name] = np.zeros_like(param)\n",
        "\n",
        "    def get_ticker_embedding(self, ticker_id):\n",
        "        \"\"\"Return the embedding for the ticker ID\"\"\"\n",
        "        return self.ticker_embedding[ticker_id].reshape(-1, 1)\n",
        "\n",
        "    def get_sector_embedding(self, sector_id):\n",
        "        \"\"\"Return the embedding for the sector ID\"\"\"\n",
        "        return self.sector_embedding[sector_id].reshape(-1, 1)\n",
        "\n",
        "    def sigmoid(self, x): return 1 / (1 + np.exp(-x))\n",
        "    def dsigmoid(self, x): return x * (1 - x)\n",
        "    def tanh(self, x): return np.tanh(x)\n",
        "    def dtanh(self, x): return 1 - x ** 2\n",
        "\n",
        "    def forward(self, x_seq, ticker_id, sector_id, h=None, c=None):\n",
        "        if h is None:\n",
        "            h = np.zeros((self.hidden_dim, 1))\n",
        "        if c is None:\n",
        "            c = np.zeros((self.hidden_dim, 1))\n",
        "        self.caches = []\n",
        "\n",
        "        # Get the ticker and sector embeddings once per sequence (not per timestep)\n",
        "        ticker_embedding = self.get_ticker_embedding(ticker_id).reshape(-1, 1)\n",
        "        sector_embedding = self.get_sector_embedding(sector_id).reshape(-1, 1)\n",
        "\n",
        "        for x in x_seq:\n",
        "            x = x.reshape(self.input_dim, 1)\n",
        "\n",
        "            # Concatenate previous hidden state, input, ticker embedding, and sector embedding\n",
        "            concat = np.vstack((h, x, ticker_embedding, sector_embedding))\n",
        "\n",
        "            ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
        "            it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
        "            c_tilde = self.tanh(np.dot(self.Wc, concat) + self.bc)\n",
        "            c = ft * c + it * c_tilde\n",
        "            ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
        "            h = ot * self.tanh(c)\n",
        "\n",
        "            self.caches.append((h, c, ft, it, c_tilde, ot, concat))\n",
        "\n",
        "        y_hat = np.dot(self.Wy, h) + self.by\n",
        "        return y_hat, h, c\n",
        "\n",
        "    def backward(self, x_seq, y_hat, y_true):\n",
        "        dh_next = np.zeros((self.hidden_dim, 1))\n",
        "        dc_next = np.zeros((self.hidden_dim, 1))\n",
        "\n",
        "        grads = {\n",
        "            'Wf': np.zeros_like(self.Wf), 'Wi': np.zeros_like(self.Wi),\n",
        "            'Wc': np.zeros_like(self.Wc), 'Wo': np.zeros_like(self.Wo),\n",
        "            'Wy': np.zeros_like(self.Wy),\n",
        "            'bf': np.zeros_like(self.bf), 'bi': np.zeros_like(self.bi),\n",
        "            'bc': np.zeros_like(self.bc), 'bo': np.zeros_like(self.bo),\n",
        "            'by': np.zeros_like(self.by)\n",
        "        }\n",
        "\n",
        "        dy = y_hat - y_true\n",
        "        grads['Wy'] += np.dot(dy, self.caches[-1][0].T)\n",
        "        grads['by'] += dy\n",
        "\n",
        "        dh = np.dot(self.Wy.T, dy) + dh_next\n",
        "\n",
        "        for t in reversed(range(len(x_seq))):\n",
        "            h, c, ft, it, c_tilde, ot, concat = self.caches[t]\n",
        "            c_prev = self.caches[t - 1][1] if t > 0 else np.zeros_like(c)\n",
        "\n",
        "            do = dh * self.tanh(c)\n",
        "            do_raw = do * self.dsigmoid(ot)\n",
        "\n",
        "            dc = dh * ot * self.dtanh(self.tanh(c)) + dc_next\n",
        "            dc_tilde = dc * it\n",
        "            dc_tilde_raw = dc_tilde * self.dtanh(c_tilde)\n",
        "\n",
        "            di = dc * c_tilde\n",
        "            di_raw = di * self.dsigmoid(it)\n",
        "\n",
        "            df = dc * c_prev\n",
        "            df_raw = df * self.dsigmoid(ft)\n",
        "\n",
        "            grads['Wf'] += np.dot(df_raw, concat.T)\n",
        "            grads['Wi'] += np.dot(di_raw, concat.T)\n",
        "            grads['Wc'] += np.dot(dc_tilde_raw, concat.T)\n",
        "            grads['Wo'] += np.dot(do_raw, concat.T)\n",
        "\n",
        "            grads['bf'] += df_raw\n",
        "            grads['bi'] += di_raw\n",
        "            grads['bc'] += dc_tilde_raw\n",
        "            grads['bo'] += do_raw\n",
        "\n",
        "            dconcat = (np.dot(self.Wf.T, df_raw) +\n",
        "                       np.dot(self.Wi.T, di_raw) +\n",
        "                       np.dot(self.Wc.T, dc_tilde_raw) +\n",
        "                       np.dot(self.Wo.T, do_raw))\n",
        "\n",
        "            dh = dconcat[:self.hidden_dim, :]\n",
        "            dc_next = dc * ft\n",
        "\n",
        "        self._apply_adam(grads)\n",
        "        self.t += 1  # Increment timestep\n",
        "\n",
        "    def _apply_adam(self, grads):\n",
        "        for param_name in grads:\n",
        "            grad = grads[param_name]\n",
        "            self.m[param_name] = self.beta1 * self.m[param_name] + (1 - self.beta1) * grad\n",
        "            self.v[param_name] = self.beta2 * self.v[param_name] + (1 - self.beta2) * (grad ** 2)\n",
        "\n",
        "            m_hat = self.m[param_name] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[param_name] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            param = getattr(self, param_name)\n",
        "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "            setattr(self, param_name, param)\n",
        "\n",
        "\n",
        "    def train(self, X_train, y_train, ticker_ids_train, sector_ids_train, epochs=10, batch_size=32):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            indices = np.arange(len(X_train))\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                batch_indices = indices[i:i + batch_size]\n",
        "                batch_loss = 0\n",
        "                batch_grads = []\n",
        "\n",
        "                for j in batch_indices:\n",
        "                    x_seq = X_train[j]\n",
        "                    y_true = y_train[j].reshape(self.output_dim, 1)\n",
        "                    ticker_ids = ticker_ids_train[j]\n",
        "                    sector_ids = sector_ids_train[j]\n",
        "                    y_hat, _, _ = self.forward(x_seq, ticker_ids, sector_ids)\n",
        "                    loss = np.mean((y_hat - y_true) ** 2)\n",
        "                    batch_loss += loss\n",
        "\n",
        "                    self.backward(x_seq, y_hat, y_true)\n",
        "\n",
        "                total_loss += batch_loss\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.6f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the sector to ID mapping\n",
        "print(sector_to_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyL808Nqh9Dx",
        "outputId": "a8ea0308-eca3-4bbc-c558-e3ac27e56764"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Technology': 0, 'Healthcare': 1, 'Financial Services': 2, 'Physical Assets & Resources': 3, 'Consumer Cyclical': 4, 'Industrials': 5, 'Communication Services': 6, 'Consumer Defensive': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sector_mapping = {\n",
        "    0: \"Technology\",\n",
        "    1: \"Healthcare\",\n",
        "    2: \"Financial Services\",\n",
        "    3: \"Physical Assets & Resources\",\n",
        "    4: \"Consumer Cyclical\",\n",
        "    5: \"Industrials\",\n",
        "    6: \"Communication Services\",\n",
        "    7: \"Consumer Defensive\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "0pZtwcDL85wa"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id, params, sector_models=None):\n",
        "    if sector_models is None:\n",
        "        sector_models = {}\n",
        "\n",
        "    # Select data for the given sector\n",
        "    sector_indices = np.where(train_sectors == sector_id)[0]\n",
        "\n",
        "    if len(sector_indices) == 0:\n",
        "        print(f\"No training data for sector {sector_id}. Skipping.\")\n",
        "        return sector_models\n",
        "\n",
        "    X_sector = X_train[sector_indices]\n",
        "    y_sector = y_train[sector_indices]\n",
        "    ticker_sector = train_tickers[sector_indices]\n",
        "    sector_sector = train_sectors[sector_indices]\n",
        "\n",
        "    # Get sector name for printing\n",
        "    sector_name = sector_mapping.get(sector_id, f\"Sector {sector_id}\")\n",
        "\n",
        "    # Check if model for sector already exists (for fine-tuning)\n",
        "    if sector_id in sector_models:\n",
        "        model = sector_models[sector_id]\n",
        "        print(f\"\\nFine-tuning sector {sector_name}...\")\n",
        "    else:\n",
        "        model = LSTM(\n",
        "            input_dim=params['input_dim'],\n",
        "            hidden_dim=params['hidden_dim'],\n",
        "            output_dim=params['output_dim'],\n",
        "            ticker_dim=params['ticker_dim'],\n",
        "            embedding_dim=params['embedding_dim'],\n",
        "            sector_dim=params['sector_dim'],\n",
        "            learning_rate=params['learning_rate']\n",
        "        )\n",
        "        print(f\"\\nTraining {sector_name} sector from scratch...\")\n",
        "\n",
        "    # Train\n",
        "    model.train(X_sector, y_sector, ticker_sector, sector_sector, epochs=params['epochs'], batch_size=params['batch_size'])\n",
        "\n",
        "    # Save the model\n",
        "    sector_models[sector_id] = model\n",
        "\n",
        "    return sector_models\n"
      ],
      "metadata": {
        "id": "OH_jHCLGyn9j"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty models\n",
        "sector_models = {}"
      ],
      "metadata": {
        "id": "kuAVTvcgBfsQ"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mj3rHm-IwEBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- TECHNOLOGY -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "tpgmAyPUwDFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Technology sector (sector_id = 0)\n",
        "technology_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 0]\n",
        "\n",
        "# Get stock names correctly\n",
        "technology_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(technology_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Technology sector:\")\n",
        "print(technology_stock_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoKh99asf0xC",
        "outputId": "243c795e-e4f7-4095-9099-063678a2d88f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stocks being trained in Technology sector:\n",
            "['AAPL', 'MSFT', 'ACN', 'ADBE', 'NVDA', 'ORCL', 'AVGO', 'IBM', 'INTC', 'INTU', 'QCOM', 'CRM', 'CSCO']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_technology = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 6,\n",
        "    'batch_size': 32\n",
        "}\n",
        "\n",
        "# Technology (sector_id -> 0)\n",
        "sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id=0, params=params_technology, sector_models=sector_models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glUyp1UBCD61",
        "outputId": "463b5cf0-58fa-45cc-b23e-c850b99e5eb7"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Technology sector from scratch...\n",
            "Epoch 1/6, Loss: 13.480283\n",
            "Epoch 2/6, Loss: 7.244335\n",
            "Epoch 3/6, Loss: 7.101977\n",
            "Epoch 4/6, Loss: 6.847715\n",
            "Epoch 5/6, Loss: 6.828608\n",
            "Epoch 6/6, Loss: 6.691818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "Bl9LOkpPahrh"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "uSe6B4AhahoJ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "bTHZS0lRahlp"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "technology_indices = [i for i, sector in enumerate(train_sectors) if sector == 0]\n",
        "\n",
        "X_technology = [X_train[i] for i in technology_indices]\n",
        "y_technology = [y_train[i] for i in technology_indices]\n",
        "tickers_technology = [train_tickers[i] for i in technology_indices]\n",
        "\n",
        "# Predict on same technology data\n",
        "log_returns_technology = predict_logreturns(sector_models[0], X_technology, tickers_technology, sector_id=0, scalers=scalers)\n",
        "\n",
        "# Calculate volatility\n",
        "volatility_technology = calculate_volatility(log_returns_technology)\n",
        "\n",
        "# Save to CSV\n",
        "output_filename = 'technology_sector_predictions.csv'\n",
        "save_predictions_to_csv(sector_id=0, log_returns=log_returns_technology, volatility=volatility_technology, test_tickers=tickers_technology, output_filename=output_filename, ticker_mapping=ticker_mapping, sector_mapping=sector_mapping)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir9kg7jXahjK",
        "outputId": "26f99de4-4cb8-4348-bd98-30e35a1fc100"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for sector 'Technology' saved to technology_sector_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4tEnB-E1ap7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- HEALTHCARE -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "YpGN3GJPap39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Health sector\n",
        "health_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 1]\n",
        "\n",
        "# Get stock names correctly\n",
        "health_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(health_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in health sector:\")\n",
        "print(health_stock_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkvlsjWQ5wrI",
        "outputId": "2ce65a9c-7903-4f7d-93d9-2f2e9ce353f5"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stocks being trained in health sector:\n",
            "['MRK', 'ABBV', 'ABT', 'AMGN', 'GILD', 'MDT', 'PFE', 'BMY', 'JNJ', 'LLY', 'CVS', 'DHR']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_healthcare = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}\n",
        "\n",
        "# Healthcare (sector_id -> 1)\n",
        "sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id=1, params=params_healthcare, sector_models=sector_models)"
      ],
      "metadata": {
        "id": "rtQ0nqkpCFRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "aU2l2Wrw6UMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "Ya3Hr8_W6UIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "d-r6tIEu6UFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health_indices = [i for i, sector in enumerate(train_sectors) if sector == 1]\n",
        "\n",
        "X_health = [X_train[i] for i in health_indices]\n",
        "y_health = [y_train[i] for i in health_indices]\n",
        "tickers_health = [train_tickers[i] for i in health_indices]\n",
        "\n",
        "log_returns_health = predict_logreturns(sector_models[0], X_health, tickers_health, sector_id=1, scalers=scalers)\n",
        "\n",
        "# Calculate volatility\n",
        "volatility_health = calculate_volatility(log_returns_health)\n",
        "\n",
        "# Save to CSV\n",
        "output_filename = 'healthcare_sector_predictions.csv'\n",
        "save_predictions_to_csv(sector_id=0, log_returns=log_returns_health, volatility=volatility_health, test_tickers=tickers_health, output_filename=output_filename, ticker_mapping=ticker_mapping, sector_mapping=sector_mapping)\n"
      ],
      "metadata": {
        "id": "PbeK_Idn6UCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HWAeSxcN6T8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- FINANCIAL SERVICES -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "TUC4nyPg6T43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Financial Services sector\n",
        "finservice_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 2]\n",
        "\n",
        "# Get stock names correctly\n",
        "finservice_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(finservice_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Financial Service sector:\")\n",
        "print(finservice_stock_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_d-9lsy6T1i",
        "outputId": "4ec66875-a55e-40a0-d392-10886387cba2"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stocks being trained in Financial Service sector:\n",
            "['MS', 'AIG', 'AXP', 'GS', 'BAC', 'BK', 'PYPL', 'BLK', 'C', 'JPM', 'COF', 'MA', 'MET']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_financial_services = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}\n",
        "\n",
        "# Financial Services (sector_id -> 2)\n",
        "sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id=2, params=params_financial_services, sector_models=sector_models)"
      ],
      "metadata": {
        "id": "lcbBlfMICFK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "SYPPxadt7yyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "OkxGfazc7ywO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "D3gftc947yuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finservice_indices = [i for i, sector in enumerate(train_sectors) if sector == 2]\n",
        "\n",
        "X_finservice = [X_train[i] for i in finservice_indices]\n",
        "y_finservice = [y_train[i] for i in finservice_indices]\n",
        "tickers_finservice = [train_tickers[i] for i in finservice_indices]\n",
        "\n",
        "log_returns_finservice = predict_logreturns(sector_models[0], X_finservice, tickers_finservice, sector_id=2, scalers=scalers)\n",
        "\n",
        "# Calculate volatility\n",
        "volatility_finservice = calculate_volatility(log_returns_finservice)\n",
        "\n",
        "# Save to CSV\n",
        "output_filename = 'FinancialService_sector_predictions.csv'\n",
        "save_predictions_to_csv(sector_id=0, log_returns=log_returns_finservice, volatility=volatility_finservice, test_tickers=tickers_finservice, output_filename=output_filename, ticker_mapping=ticker_mapping, sector_mapping=sector_mapping)\n"
      ],
      "metadata": {
        "id": "sXvdSQFe7yre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e5-MV61e7ypa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- PHYSICAL ASSETS -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "_0Hsj2_J7ynH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Physical Assets & Resources sector\n",
        "phyassets_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 3]\n",
        "\n",
        "# Get stock names correctly\n",
        "phyassets_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(phyassets_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Physical Assets sector:\")\n",
        "print(phyassets_stock_names)\n"
      ],
      "metadata": {
        "id": "blMESR737yk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_physical_assets_resources = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}\n",
        "\n",
        "# Physical Assets & Resources (sector_id -> 3)\n",
        "sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id=3, params=params_physical_assets_resources, sector_models=sector_models)"
      ],
      "metadata": {
        "id": "hlsvnUMZCFEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "gm4LVEcA82Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "6CLev4oq81_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "o83q3IW38156"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phyassets_indices = [i for i, sector in enumerate(train_sectors) if sector == 3]\n",
        "\n",
        "X_phyassets = [X_train[i] for i in phyassets_indices]\n",
        "y_phyassets = [y_train[i] for i in phyassets_indices]\n",
        "tickers_phyassets = [train_tickers[i] for i in phyassets_indices]\n",
        "\n",
        "log_returns_phyassets = predict_logreturns(sector_models[0], X_phyassets, tickers_phyassets, sector_id=3, scalers=scalers)\n",
        "\n",
        "# Calculate volatility\n",
        "volatility_phyassets = calculate_volatility(log_returns_phyassets)\n",
        "\n",
        "# Save to CSV\n",
        "output_filename = 'PhyscicalAssets_sector_predictions.csv'\n",
        "save_predictions_to_csv(sector_id=0, log_returns=log_returns_phyassets, volatility=volatility_phyassets, test_tickers=tickers_phyassets, output_filename=output_filename, ticker_mapping=ticker_mapping, sector_mapping=sector_mapping)\n"
      ],
      "metadata": {
        "id": "XzRYEyVh811F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mCrKII9C81xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- CONSUMER CYCLIC -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "Oo9pllkb81qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Consumer Cyclical sector\n",
        "concyclic_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 4]\n",
        "\n",
        "# Get stock names correctly\n",
        "concyclic_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(concyclic_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Consumer Cyclic sector:\")\n",
        "print(concyclic_stock_names)\n"
      ],
      "metadata": {
        "id": "36SisDOL81ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_consumer_cyclical = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}\n",
        "\n",
        "# Consumer Cyclical (sector_id -> 4)\n",
        "sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id=4, params=params_consumer_cyclical, sector_models=sector_models)"
      ],
      "metadata": {
        "id": "oc4gsheyCE-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "c4VEfldC-E44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "Je153DE2-EzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "STBEYmto-Etk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concyclic_indices = [i for i, sector in enumerate(train_sectors) if sector == 4]\n",
        "\n",
        "X_concyclic = [X_train[i] for i in concyclic_indices]\n",
        "y_concyclic = [y_train[i] for i in concyclic_indices]\n",
        "tickers_concyclic = [train_tickers[i] for i in concyclic_indices]\n",
        "\n",
        "log_returns_concyclic = predict_logreturns(sector_models[0], X_concyclic, tickers_concyclic, sector_id=4, scalers=scalers)\n",
        "\n",
        "# Calculate volatility\n",
        "volatility_concyclic = calculate_volatility(log_returns_concyclic)\n",
        "\n",
        "# Save to CSV\n",
        "output_filename = 'ConsumerCyclic_sector_predictions.csv'\n",
        "save_predictions_to_csv(sector_id=0, log_returns=log_returns_concyclic, volatility=volatility_concyclic, test_tickers=tickers_concyclic, output_filename=output_filename, ticker_mapping=ticker_mapping, sector_mapping=sector_mapping)\n"
      ],
      "metadata": {
        "id": "VoQfNtDB-Ent"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IU-qRLBZ-4oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- INDUSTRIALS -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "LCn2EL5S-Ehq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Industrial sector\n",
        "industrial_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 5]\n",
        "\n",
        "# Get stock names correctly\n",
        "industrial_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(industrial_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Industrial sector:\")\n",
        "print(industrial_stock_names)\n"
      ],
      "metadata": {
        "id": "GXD8UUDU-Eac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_industrials = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}\n",
        "\n",
        "# Industrials (sector_id -> 5)\n",
        "sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id=5, params=params_industrials, sector_models=sector_models)"
      ],
      "metadata": {
        "id": "NawPCmimCE4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "WvxwP30g_Oxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "aXvGd9pL_OmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "Bpwv_YwT_Ofw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "industrial_indices = [i for i, sector in enumerate(train_sectors) if sector == 5]\n",
        "\n",
        "X_industrial = [X_train[i] for i in industrial_indices]\n",
        "y_industrial = [y_train[i] for i in industrial_indices]\n",
        "tickers_industrial = [train_tickers[i] for i in industrial_indices]\n",
        "\n",
        "log_returns_industrial = predict_logreturns(sector_models[0], X_industrial, tickers_industrial, sector_id=4, scalers=scalers)\n",
        "\n",
        "# Calculate volatility\n",
        "volatility_industrial = calculate_volatility(log_returns_industrial)\n",
        "\n",
        "# Save to CSV\n",
        "output_filename = 'Industrial_sector_predictions.csv'\n",
        "save_predictions_to_csv(sector_id=0, log_returns=log_returns_industrial, volatility=volatility_industrial, test_tickers=tickers_industrial, output_filename=output_filename, ticker_mapping=ticker_mapping, sector_mapping=sector_mapping)\n"
      ],
      "metadata": {
        "id": "0oPGmspw_OZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qn366WO6_OTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- COMMUNICATION SERVICES -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "bA8KmlxI_OM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Communication Services sector\n",
        "commserv_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 6]\n",
        "\n",
        "# Get stock names correctly\n",
        "commserv_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(commserv_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Communication Services sector:\")\n",
        "print(commserv_stock_names)\n"
      ],
      "metadata": {
        "id": "-LqaRVgM_OFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_communication_servicess = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}\n",
        "\n",
        "# Communication Services (sector_id -> 6)\n",
        "sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id=6, params=params_communication_servicess, sector_models=sector_models)"
      ],
      "metadata": {
        "id": "eE8ttvIZCEwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "Vb-2GlODAQjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "h3jXUJRSAPaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "lGgQMcb8APDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commserv_indices = [i for i, sector in enumerate(train_sectors) if sector == 6]\n",
        "\n",
        "X_commserv = [X_train[i] for i in commserv_indices]\n",
        "y_commserv = [y_train[i] for i in commserv_indices]\n",
        "tickers_commserv = [train_tickers[i] for i in commserv_indices]\n",
        "\n",
        "log_returns_commserv = predict_logreturns(sector_models[0], X_commserv, tickers_commserv, sector_id=4, scalers=scalers)\n",
        "\n",
        "# Calculate volatility\n",
        "volatility_commserv = calculate_volatility(log_returns_commserv)\n",
        "\n",
        "# Save to CSV\n",
        "output_filename = 'CommunicationServices_sector_predictions.csv'\n",
        "save_predictions_to_csv(sector_id=0, log_returns=log_returns_commserv, volatility=volatility_commserv, test_tickers=tickers_commserv, output_filename=output_filename, ticker_mapping=ticker_mapping, sector_mapping=sector_mapping)\n"
      ],
      "metadata": {
        "id": "e3AhDqLvAPAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAdEg5DzAO-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- CONSUMER DEFENCE -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "nhY-fXmtAO1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Consumer Defence sector\n",
        "consdef_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 7]\n",
        "\n",
        "# Get stock names correctly\n",
        "consdef_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(consdef_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Consumer Defence sector:\")\n",
        "print(consdef_stock_names)\n"
      ],
      "metadata": {
        "id": "Mp7AUVzFAOuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_consumer_defensive = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}\n",
        "\n",
        "# Consumer Defensive (sector_id -> 7)\n",
        "sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id=7, params=params_consumer_defensive, sector_models=sector_models)"
      ],
      "metadata": {
        "id": "vjIoGvocDHJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "JZgE5rsoBRLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "hvq4fVzaBRIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "-O01ErQYBRA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "consdef_indices = [i for i, sector in enumerate(train_sectors) if sector == 7]\n",
        "\n",
        "X_consdef= [X_train[i] for i in consdef_indices]\n",
        "y_consdef = [y_train[i] for i in consdef_indices]\n",
        "tickers_consdef = [train_tickers[i] for i in consdef_indices]\n",
        "\n",
        "log_returns_consdef = predict_logreturns(sector_models[0], X_consdef, tickers_consdef, sector_id=4, scalers=scalers)\n",
        "\n",
        "# Calculate volatility\n",
        "volatility_consdef = calculate_volatility(log_returns_consdef)\n",
        "\n",
        "# Save to CSV\n",
        "output_filename = 'ConsumerDefence_sector_predictions.csv'\n",
        "save_predictions_to_csv(sector_id=0, log_returns=log_returns_consdef, volatility=volatility_consdef, test_tickers=tickers_consdef, output_filename=output_filename, ticker_mapping=ticker_mapping, sector_mapping=sector_mapping)\n"
      ],
      "metadata": {
        "id": "KoDzfBopBQ6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rb56MJX6BQzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRoUflWfynvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7YwmlmZafPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_stocks_in_sector(train_tickers, train_sectors, test_tickers, test_sectors, ticker_mapping, sector_id, sector_mapping):\n",
        "    \"\"\"\n",
        "    Prints the unique stocks in train and test data for a given sector.\n",
        "    \"\"\"\n",
        "    # Reverse mapping: ID -> Ticker Name\n",
        "    ticker_to_id = {v: k for k, v in ticker_mapping.items()}\n",
        "\n",
        "    # Get tickers for the sector in train and test\n",
        "    train_sector_tickers = [ticker for ticker, sector in zip(train_tickers, train_sectors) if sector == sector_id]\n",
        "    test_sector_tickers = [ticker for ticker, sector in zip(test_tickers, test_sectors) if sector == sector_id]\n",
        "\n",
        "    unique_train_tickers = set(train_sector_tickers)\n",
        "    unique_test_tickers = set(test_sector_tickers)\n",
        "\n",
        "    # Map ticker IDs back to names\n",
        "    train_stock_names = [id_to_ticker[ticker_id] for ticker_id in unique_train_tickers]\n",
        "    test_stock_names = [id_to_ticker[ticker_id] for ticker_id in unique_test_tickers]\n",
        "\n",
        "    sector_name = sector_mapping.get(sector_id, f\"Sector {sector_id}\")\n",
        "\n",
        "    print(f\"\\n--- {sector_name} ---\")\n",
        "    print(f\"Unique stocks in train data ({len(train_stock_names)} stocks): {sorted(train_stock_names)}\")\n",
        "    print(f\"Unique stocks in test data ({len(test_stock_names)} stocks): {sorted(test_stock_names)}\")\n"
      ],
      "metadata": {
        "id": "i5myD4SrafMJ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for Technology (sector_id = 0)\n",
        "print_stocks_in_sector(train_tickers, train_sectors, test_tickers, test_sectors, ticker_mapping, sector_id=0, sector_mapping=sector_mapping)\n"
      ],
      "metadata": {
        "id": "67KS950gFLEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "# Filter tickers belonging to Technology sector (sector_id = 0)\n",
        "technology_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 0]\n",
        "\n",
        "# Get stock names correctly\n",
        "technology_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(technology_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Technology sector:\")\n",
        "print(technology_stock_names)"
      ],
      "metadata": {
        "id": "cqdieB0JFMm3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}