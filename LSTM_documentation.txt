************* Long Short Term Memory Documentation *************  

LSTM_v1(vanilla).ipynb
Basic Lstm model that predicts stock prices based on closing prices.
With default parameters:
1) input_dim
2) hidden_dim
3) output_dim
4) learning_rate

Now i have yet to add other parameter like:  
1) dropout(regularization)
2) Adam optimizer which works well for time-series
3) sequence length (capture short/long term trends)
4) batch size
5) Bidirectional

i will also implement feature engineering. for now the model is only considering
closing prices but i will add:
1) moving averages (SMA, EMA)
2) trading volume
3) relative strength index (RSI)
4) moving average convergence/divergence (MACD)
5) Bollinger Bands (maybe not sure)


LSTM_v2.ipynb
Mid tier model that predicts stock prices based on closing prices (still).
With new additional parameters:
1) input_dim
2) hidden_dim
3) output_dim
4) learning_rate
5) dropout
6) adam optimiser
7) sequence length (pichle kitne dinon ka data chahiye for training)

*Next step will be implementing feature engineering as i mentioned 
above with maybe batch size implentation

*There is also some issue while training since the loss inc/dec randomly. 
will fix it as well (issue most probably is that weights arent being updated properly)

i tried implementing bi-directional lstm with same param as before. the code works but the accuracy is really bad.
i am thinking to use lstm_v1(vanilla).ipynb file since it is the basic lstm code and then make changes in it again including feature engineering
since this code gave me best stock price prediction, i will also add adam optimiser to adapt to trends.


LSTM_v3.ipynb:
i used the original lstm_v1(vanilla).ipynb code. it is a vanilla lstm model.

**vanilla lstm model:
a vanilla LSTM (Long Short-Term Memory) is the basic version of an LSTM neural network. 
it is a type of Recurrent Neural Network (RNN) specifically designed to learn long-term dependencies
and solve the vanishing gradient problem that basic RNNs face.

i then implemented backpropagation through time (BPTT) and thr weightsare being updated in both the forward pass
and the backward pass. For now i am using stochastic gradient descent (SGD) but now in this code i will implement
ADAM optimiser for better adaptive learning rates for both forward and backward path and additionally 
add dropout rate so that randomly any neuron can be dropped so that model doesnt overfit or underfit.
typically i believe our model should overfit since in my experience the stock prices either follow three stages:
1) uptrend 2) sideways movement 3) downtrend
in all these cases prices dont fluctuate rapidly so overfitting will do the job unless there is some other factors like
news, company's financial report or any other reason which makes the stock prices go up or down rapidly


LSTM_v3_withGraphs.ipynb
i added some graphs in this notebook and fixed sequence error. the model was being trained on single last sequence but in 
def create_sequences we were training for 50 days. because of training on single last sequence, the prediction was starting from 0 
which is abnormal but when fixed (LSTM being trained on 50 sequential days), stock prices prediction was correct. 


LSTM_stock_feature_engineering.ipynb
since our LSTM_v3_withGraphs.ipynb is working correctly with correct backpropagation through time implementation (vanilla)
so now we need to add more input gates to make the model more robust and accurate. Instead of tempering with LSTM_v3_withGraphs.ipynb
i created this new file LSTM_stock_feature_engineering.ipynb where i save the stock data imported from yfinance into <ticker>_data.csv 
then load the csv, perform some feature engineering:
1) Moving averages (10)
   Moving averages (50)
   Moving averages (100)
   Moving averages (200)
2) Relative Strength Index (RSI) for 14 days 
3) Normalized Volume (range 0-1)
4) Bollinger Band Calculation

i then added these features in <ticker>_stock_features.csv
i plotted rsi and Bollinger band graph jus to check if they are working fine

*now the next step is to import this data set first in LSTM_v3_withGraphs.ipynb, normalize each feature (volume is already done).
this way we will have 4/5 input features [closing price, moving avgs, rsi, volume and bollinger band(maybe)]
we will only have to make small changes in LSTM_v3_withGraphs.ipynb specifically:
1) class LSTM
2) def create_sequences
3) def train
4) def predict

LSTM_integration_features.ipynb
This code is a LSTM_v3_withGraphs.ipynb wrapper. i just imported the <ticker>_stock_features.csv and then used the features 
[close price(already implemented), RSI, normalized volume, moving averages, bollinger Bands] as input. so now the LSTM model
is no more vanilla model since now we 5 input gates. 
for the selection of Moving Averages, i chose MA-100 since used to identify medium-term trends and can be used to confirm
or deny the trend identified by shorter or longer MAs Can be used to identify support and resistance levels 
After updating code accordingly, the code is working perfectly. 
i used 100 epochs with learning rate 0.01 and loss was 0.9. although when i was using vanilla lstm or vanilla lstm with BPTT
the accuracy was 0.6 with same parameters but there was only one input gate. if i decrease learning rate to 0.005 and increase epochs 
to 200-300, accuracy of 0.7-0.6 will be easy achievable. 

*next step is to implement more parameters in LSTM_integration_features.ipynb
1) adam optimiser
2) dropout rate (important)
3) batch size
4) Bidirectional lstm (already implemented in BPTT)


* the work from here on is done on LSTM_integration_features.ipynb

LSTM_integration_features_dropout_batchsize.ipynb
adding both dropout and batch size param is not working well. There is a lot of variation when using both together.
LSTMs are good for long terms but an important thing about them is that they are not very well at memorising multiple
things simultaneously. The logic of drop out is for adding noise to the neurons in order not to be dependent on any 
specific neuron. By adding drop out for LSTM cells, there is a chance for forgetting something that should not be forgotten.

* i will now try removing drop out and only implement batch size

LSTM_integration_features_batchsize.ipynb
in LSTM_integration_features.ipynb, i added batch size as parameter and the results are slightly better but
not that significant. keeping LSTM_integration_features.ipynb or just adding batch size as parameter wont make such
a drastic increase in accuracy of stock prediction.
i will keep this parameter since even a small increase in accuracy while predicting stock prices in win-win.

* i will now try adam optimiser for weight initialisation instead of xavier which we did in LSTM_integration_features.ipynb

LSTM_integration_features_adam.ipynb
the issue with adam optimiser for weight initialisation is that the prediction of stock is way less than actual
even when i multiply the weights by 0.1 or 0.05 other than 0.01. if i use large enough number then it would lead to 
exploding gradients *(gradients become excessively large during backpropagation, potentially causing the
network to become unstable and fail to learn effectively)

* i then experimented with adam optimiser and batch size

LSTM_integration_features_adam_batchsize.ipynb
i integrated both adam optimiser and batch size and the code runs accurately. the predicted stock values are much more closer
to the actual stock price

* i still need to try more experimentation of batch size, adam optimiser and xavier initialisation on other stocks to
see which parameters works best. 

* i used microsoft stock (MSFT) for the model 1) LSTM_integration_features_batchsize.ipynb and 2) LSTM_integration_features_adam_batchsize.ipynb
LSTM_integration_features_batchsize.ipynb gave a good accuracy but it wsa not precise. the predicted stock price was lower than actual for all values
LSTM_integration_features_adam_batchsize.ipynb gave good accuracy and good score.

* so the only two models that provide accurate and precise results are:
1) LSTM_integration_features_adam_batchsize.ipynb
2) LSTM_integration_features.ipynb (uses xavier initialisation)

* i created a new file : LSTM_integration_features_adam_batchsize (final).ipynb
i integrated the output which was required in section 3.3 of the proposal which were
stock return and volatility. 

the final .csv file is located in CSVs folder (results_with_errors.csv)

*now the potential issue with finalised LSTM model is that i have to train model for each stock which means different values
for each parameters(close price, volume, rsi, MA, bollinger bands). because of this i would have to manually set parameters
for each stock and even then i wouldnt know if that model will predict correctly so a lot of time will be put in optimising the 
model for each stock

*for this i am using a library based LSTM model (implementation of my finalised scratch LSTM code through libraries)
i might use optuna for parameter tuning for better prediction.


for now i created a FINAL_LSTM model where i will create the final model. i have created a stock_feature_engineering.ipynb 
which has feature engineering pipeline. it uses s&p 100 index as its base core and use the data of these 100 companies.
it has two folders:
1) basic_CSVs: CSVs of 100 stocks with basic features like close price, day high, day low etc
2) detailed_CSVs: CSVs of 100 stocks with detailed features like RSI, BB, MA, Pct_Change, Log_Returns etc
After this, i have created a combined_stock_features.csv which has all the data from detailed_CSVs in it with ticker name as well

This combined_stock_features.csv will be used as the input to the LSTM model i created (scratch or library version idk yet)
it will be given these inputs and maybe ticker to learn - a more generalizable model.

i trained the model on the entire csv with ticker embeddings (label encoding). i preferred label encoding over one-hot encoding
since one-hot encoding would result in matrix of dimension 100. now the issue with this code is that it takes a lot of time to run
since i have 79,087 rows , it will take around 12+ hours to run, what i can do now is reduce epoch size from 150 to 50, or
use JAX implementation which is specifically designed for numpy Calculations, there will be few changes in code but code will
run faster. 

first i will check running the code for 50 epochs and see how fast does it run but if it is taking a lot of time, i will use 
JAX implementation


*HOW TO RUN THE SETUP*
1)look into LSTM_v1(vanilla).ipynb, LSTM_v2, LSTM_v3_withGraphs (DONT RUN)
2)run LSTM_stock_feature_engineering.ipynb
3)run LSTM_integration_features.ipynb (uses xavier initialisation)
4)run LSTM_integration_features_adam_batchsize.ipynb


