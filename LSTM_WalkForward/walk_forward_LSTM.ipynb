{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "rzYJKYVjJIyl",
        "outputId": "8cf0b7e6-9df7-4884-a4f9-80049c4e33b6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-78059009-7ad9-4a38-b1ca-96497c4a556f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-78059009-7ad9-4a38-b1ca-96497c4a556f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving combined_stock_features.csv to combined_stock_features.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Click \"Choose Files\" and select your products.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvpFyqFiL3Z0",
        "outputId": "fdfbc68b-3855-4096-996f-9b14879030f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Date       Close        High         Low        Open  \\\n",
            "0      2023-01-03  123.632523  129.395510  122.742865  128.782641   \n",
            "1      2023-01-04  124.907700  127.181268  123.642412  125.431607   \n",
            "2      2023-01-05  123.583107  126.301500  123.326101  125.668857   \n",
            "3      2023-01-06  128.130203  128.792501  123.454572  124.561702   \n",
            "4      2023-01-09  128.654144  131.876686  128.397138  128.970474   \n",
            "...           ...         ...         ...         ...         ...   \n",
            "49191  2024-12-24  105.456718  106.239716  104.762920  105.575650   \n",
            "49192  2024-12-26  105.545921  106.081135  105.000802  105.575654   \n",
            "49193  2024-12-27  105.536011  107.032618  104.832299  105.357606   \n",
            "49194  2024-12-30  104.822395  105.615298  104.574612  105.357609   \n",
            "49195  2024-12-31  106.616348  106.943424  104.842216  105.228758   \n",
            "\n",
            "            Volume        MA10        MA50       MA100       MA200        RSI  \\\n",
            "0      112117500.0  128.989240  141.020708  146.761524  149.348865  22.617030   \n",
            "1       89113600.0  128.395150  140.612136  146.347824  149.166577  23.287613   \n",
            "2       80962700.0  127.675517  140.134045  145.885252  148.970778  23.902384   \n",
            "3       87754700.0  127.099216  139.689855  145.457395  148.780788  39.492886   \n",
            "4       70790800.0  126.893607  139.315159  145.036356  148.586579  43.030652   \n",
            "...            ...         ...         ...         ...         ...        ...   \n",
            "49191    7807000.0  107.197153  115.159559  115.028485  113.731151  13.039350   \n",
            "49192    9652400.0  106.658967  114.904260  114.964701  113.734965   9.560926   \n",
            "49193   11943900.0  106.129701  114.642668  114.906766  113.732877  10.828675   \n",
            "49194   11080800.0  105.626205  114.372898  114.826873  113.717837  10.769181   \n",
            "49195   12387800.0  105.537003  114.145693  114.743366  113.712735  26.123565   \n",
            "\n",
            "       Norm_Volume  Bollinger_Upper  Bollinger_Lower  Log_Returns  Pct_Change  \\\n",
            "0         1.331549       148.054167       121.250636    -0.038122   -0.037405   \n",
            "1         1.057702       146.806192       120.494904     0.010261    0.010314   \n",
            "2         0.959823       146.141765       119.390893    -0.010661   -0.010605   \n",
            "3         1.037141       145.362307       119.051356     0.036133    0.036794   \n",
            "4         0.840106       144.150863       119.027169     0.004081    0.004089   \n",
            "...            ...              ...              ...          ...         ...   \n",
            "49191     0.515993       119.938902       101.963248     0.000940    0.000941   \n",
            "49192     0.643165       119.217199       101.547128     0.000846    0.000846   \n",
            "49193     0.792140       118.402853       101.253386    -0.000094   -0.000094   \n",
            "49194     0.735019       117.389984       101.057070    -0.006785   -0.006762   \n",
            "49195     0.822273       116.125416       101.302753     0.016969    0.017114   \n",
            "\n",
            "      Ticker                       Sector  \n",
            "0       AAPL                   Technology  \n",
            "1       AAPL                   Technology  \n",
            "2       AAPL                   Technology  \n",
            "3       AAPL                   Technology  \n",
            "4       AAPL                   Technology  \n",
            "...      ...                          ...  \n",
            "49191    XOM  Physical Assets & Resources  \n",
            "49192    XOM  Physical Assets & Resources  \n",
            "49193    XOM  Physical Assets & Resources  \n",
            "49194    XOM  Physical Assets & Resources  \n",
            "49195    XOM  Physical Assets & Resources  \n",
            "\n",
            "[49196 rows x 18 columns]\n",
            "             Date       Close        High         Low        Open  \\\n",
            "0      2023-01-03  123.632523  129.395510  122.742865  128.782641   \n",
            "1      2023-01-04  124.907700  127.181268  123.642412  125.431607   \n",
            "2      2023-01-05  123.583107  126.301500  123.326101  125.668857   \n",
            "3      2023-01-06  128.130203  128.792501  123.454572  124.561702   \n",
            "4      2023-01-09  128.654144  131.876686  128.397138  128.970474   \n",
            "...           ...         ...         ...         ...         ...   \n",
            "49191  2024-12-24  105.456718  106.239716  104.762920  105.575650   \n",
            "49192  2024-12-26  105.545921  106.081135  105.000802  105.575654   \n",
            "49193  2024-12-27  105.536011  107.032618  104.832299  105.357606   \n",
            "49194  2024-12-30  104.822395  105.615298  104.574612  105.357609   \n",
            "49195  2024-12-31  106.616348  106.943424  104.842216  105.228758   \n",
            "\n",
            "            Volume        MA10        MA50       MA100       MA200        RSI  \\\n",
            "0      112117500.0  128.989240  141.020708  146.761524  149.348865  22.617030   \n",
            "1       89113600.0  128.395150  140.612136  146.347824  149.166577  23.287613   \n",
            "2       80962700.0  127.675517  140.134045  145.885252  148.970778  23.902384   \n",
            "3       87754700.0  127.099216  139.689855  145.457395  148.780788  39.492886   \n",
            "4       70790800.0  126.893607  139.315159  145.036356  148.586579  43.030652   \n",
            "...            ...         ...         ...         ...         ...        ...   \n",
            "49191    7807000.0  107.197153  115.159559  115.028485  113.731151  13.039350   \n",
            "49192    9652400.0  106.658967  114.904260  114.964701  113.734965   9.560926   \n",
            "49193   11943900.0  106.129701  114.642668  114.906766  113.732877  10.828675   \n",
            "49194   11080800.0  105.626205  114.372898  114.826873  113.717837  10.769181   \n",
            "49195   12387800.0  105.537003  114.145693  114.743366  113.712735  26.123565   \n",
            "\n",
            "       Norm_Volume  Bollinger_Upper  Bollinger_Lower  Log_Returns  Pct_Change  \\\n",
            "0         1.331549       148.054167       121.250636    -0.038122   -0.037405   \n",
            "1         1.057702       146.806192       120.494904     0.010261    0.010314   \n",
            "2         0.959823       146.141765       119.390893    -0.010661   -0.010605   \n",
            "3         1.037141       145.362307       119.051356     0.036133    0.036794   \n",
            "4         0.840106       144.150863       119.027169     0.004081    0.004089   \n",
            "...            ...              ...              ...          ...         ...   \n",
            "49191     0.515993       119.938902       101.963248     0.000940    0.000941   \n",
            "49192     0.643165       119.217199       101.547128     0.000846    0.000846   \n",
            "49193     0.792140       118.402853       101.253386    -0.000094   -0.000094   \n",
            "49194     0.735019       117.389984       101.057070    -0.006785   -0.006762   \n",
            "49195     0.822273       116.125416       101.302753     0.016969    0.017114   \n",
            "\n",
            "      Ticker                       Sector  \n",
            "0       AAPL                   Technology  \n",
            "1       AAPL                   Technology  \n",
            "2       AAPL                   Technology  \n",
            "3       AAPL                   Technology  \n",
            "4       AAPL                   Technology  \n",
            "...      ...                          ...  \n",
            "49191    XOM  Physical Assets & Resources  \n",
            "49192    XOM  Physical Assets & Resources  \n",
            "49193    XOM  Physical Assets & Resources  \n",
            "49194    XOM  Physical Assets & Resources  \n",
            "49195    XOM  Physical Assets & Resources  \n",
            "\n",
            "[49196 rows x 18 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Read the uploaded file\n",
        "df = pd.read_csv(io.BytesIO(uploaded['combined_stock_features.csv']))\n",
        "print(df)\n",
        "\n",
        "# Or if you just want to save it first:\n",
        "with open('combined_stock_features.csv', 'wb') as f:\n",
        "    f.write(uploaded['combined_stock_features.csv'])\n",
        "\n",
        "df = pd.read_csv('combined_stock_features.csv')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "58rbigxyL6Tm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXzxPWQqMCZi"
      },
      "outputs": [],
      "source": [
        "# # Load the enhanced dataset\n",
        "# def load_enhanced_data(ticker=\"MSFT\"):\n",
        "#     df = pd.read_csv(f\"{ticker}_stock_features.csv\")\n",
        "#     df['Date'] = pd.to_datetime(df['Date'])  # Ensure Date is datetime\n",
        "#     df.set_index('Date', inplace=True)\n",
        "#     return df\n",
        "\n",
        "# ticker = \"MSFT\"\n",
        "# data = load_enhanced_data(ticker)\n",
        "# print(data.head())  # Verify all features are loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "874NpZVLpug4"
      },
      "outputs": [],
      "source": [
        "# Create a mapping for Ticker and Sector\n",
        "ticker_to_id = {ticker: idx for idx, ticker in enumerate(df['Ticker'].unique())}\n",
        "sector_to_id = {sector: idx for idx, sector in enumerate(df['Sector'].unique())}\n",
        "\n",
        "# Add Ticker_ID and Sector_ID columns\n",
        "df['Ticker_ID'] = df['Ticker'].map(ticker_to_id)\n",
        "df['Sector_ID'] = df['Sector'].map(sector_to_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ticker_to_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFF3QpeVm7wO",
        "outputId": "2a000bce-a05a-48fe-b710-02b559179a17"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AAPL': 0,\n",
              " 'ABBV': 1,\n",
              " 'ABT': 2,\n",
              " 'ACN': 3,\n",
              " 'ADBE': 4,\n",
              " 'AIG': 5,\n",
              " 'AMGN': 6,\n",
              " 'AMT': 7,\n",
              " 'AMZN': 8,\n",
              " 'AVGO': 9,\n",
              " 'AXP': 10,\n",
              " 'BA': 11,\n",
              " 'BAC': 12,\n",
              " 'BK': 13,\n",
              " 'BKNG': 14,\n",
              " 'BLK': 15,\n",
              " 'BMY': 16,\n",
              " 'C': 17,\n",
              " 'CAT': 18,\n",
              " 'CHTR': 19,\n",
              " 'CL': 20,\n",
              " 'CMCSA': 21,\n",
              " 'COF': 22,\n",
              " 'COP': 23,\n",
              " 'COST': 24,\n",
              " 'CRM': 25,\n",
              " 'CSCO': 26,\n",
              " 'CVS': 27,\n",
              " 'CVX': 28,\n",
              " 'DHR': 29,\n",
              " 'DIS': 30,\n",
              " 'DOW': 31,\n",
              " 'DUK': 32,\n",
              " 'EMR': 33,\n",
              " 'EXC': 34,\n",
              " 'F': 35,\n",
              " 'FDX': 36,\n",
              " 'GD': 37,\n",
              " 'GE': 38,\n",
              " 'GILD': 39,\n",
              " 'GM': 40,\n",
              " 'GOOG': 41,\n",
              " 'GS': 42,\n",
              " 'HD': 43,\n",
              " 'HON': 44,\n",
              " 'IBM': 45,\n",
              " 'INTC': 46,\n",
              " 'INTU': 47,\n",
              " 'JNJ': 48,\n",
              " 'JPM': 49,\n",
              " 'KHC': 50,\n",
              " 'KO': 51,\n",
              " 'LIN': 52,\n",
              " 'LLY': 53,\n",
              " 'LMT': 54,\n",
              " 'LOW': 55,\n",
              " 'MA': 56,\n",
              " 'MCD': 57,\n",
              " 'MDLZ': 58,\n",
              " 'MDT': 59,\n",
              " 'MET': 60,\n",
              " 'META': 61,\n",
              " 'MMM': 62,\n",
              " 'MO': 63,\n",
              " 'MRK': 64,\n",
              " 'MS': 65,\n",
              " 'MSFT': 66,\n",
              " 'NEE': 67,\n",
              " 'NFLX': 68,\n",
              " 'NKE': 69,\n",
              " 'NVDA': 70,\n",
              " 'ORCL': 71,\n",
              " 'PEP': 72,\n",
              " 'PFE': 73,\n",
              " 'PG': 74,\n",
              " 'PM': 75,\n",
              " 'PYPL': 76,\n",
              " 'QCOM': 77,\n",
              " 'RTX': 78,\n",
              " 'SBUX': 79,\n",
              " 'SO': 80,\n",
              " 'SPG': 81,\n",
              " 'T': 82,\n",
              " 'TGT': 83,\n",
              " 'TMO': 84,\n",
              " 'TMUS': 85,\n",
              " 'TSLA': 86,\n",
              " 'TXN': 87,\n",
              " 'UNH': 88,\n",
              " 'UNP': 89,\n",
              " 'UPS': 90,\n",
              " 'USB': 91,\n",
              " 'V': 92,\n",
              " 'VZ': 93,\n",
              " 'WBA': 94,\n",
              " 'WFC': 95,\n",
              " 'WMT': 96,\n",
              " 'XOM': 97}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sector_to_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d23RYalFsddB",
        "outputId": "f745a153-5b21-4723-9a9a-173943ad7d19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Technology': 0,\n",
              " 'Healthcare': 1,\n",
              " 'Financial Services': 2,\n",
              " 'Physical Assets & Resources': 3,\n",
              " 'Consumer Cyclical': 4,\n",
              " 'Industrials': 5,\n",
              " 'Communication Services': 6,\n",
              " 'Consumer Defensive': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oi8WkXezMHFR"
      },
      "outputs": [],
      "source": [
        "# Select features and target\n",
        "features = ['Close', 'MA100', 'RSI', 'Norm_Volume', 'Bollinger_Upper', 'Bollinger_Lower', 'Log_Returns', 'Ticker_ID', 'Sector_ID']\n",
        "data = df[features]  # Keep only selected features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQDJpNhJjMW6",
        "outputId": "8a59405b-be28-4e7d-ed6e-761d57e4e1a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data['Sector_ID'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7B1D-B29MIKN"
      },
      "outputs": [],
      "source": [
        "def min_max_scaling(data):\n",
        "    \"\"\"Scale data to [0, 1] range and return scaled data + min/max values\"\"\"\n",
        "    min_val = np.min(data)\n",
        "    max_val = np.max(data)\n",
        "    # Handle case where all values are identical (avoid division by zero)\n",
        "    if max_val == min_val:\n",
        "        return np.zeros_like(data), min_val, max_val\n",
        "    return (data - min_val) / (max_val - min_val), min_val, max_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R1fDmlHWMJvB"
      },
      "outputs": [],
      "source": [
        "def scale_features(df):\n",
        "    scaled_data = {}\n",
        "    scalers = {}\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col in ['Ticker_ID', 'Sector_ID']:  # Don't scale these columns\n",
        "            scaled_data[col] = df[col].values\n",
        "            continue\n",
        "\n",
        "        scaled_values, min_val, max_val = min_max_scaling(df[col].values)\n",
        "        if col == 'Log_Returns':\n",
        "            scalers['target'] = (min_val, max_val)\n",
        "        else:\n",
        "            scalers[col] = (min_val, max_val)\n",
        "\n",
        "        scaled_data[col] = scaled_values\n",
        "\n",
        "    return pd.DataFrame(scaled_data), scalers\n",
        "\n",
        "# Scale all features\n",
        "scaled_data, scalers = scale_features(data)\n",
        "\n",
        "# Split into train/test (keeping temporal order)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(scaled_data) * split_ratio)\n",
        "train_data = scaled_data.iloc[:split_index]\n",
        "test_data = scaled_data.iloc[split_index:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGHXXjsMzRN1",
        "outputId": "bca265db-093e-404d-d535-53b340ada97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Close', 'MA100', 'RSI', 'Norm_Volume', 'Bollinger_Upper',\n",
            "       'Bollinger_Lower', 'Log_Returns', 'Ticker_ID', 'Sector_ID'],\n",
            "      dtype='object')\n",
            "Index(['Close', 'MA100', 'RSI', 'Norm_Volume', 'Bollinger_Upper',\n",
            "       'Bollinger_Lower', 'Log_Returns', 'Ticker_ID', 'Sector_ID'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(train_data.columns)\n",
        "print(test_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eaeTYKXqxYXV"
      },
      "outputs": [],
      "source": [
        "# Create ticker mapping and ticker IDs\n",
        "unique_tickers = list(set(train_data['Ticker_ID'].values) | set(test_data['Ticker_ID'].values))\n",
        "ticker_mapping = {ticker: idx for idx, ticker in enumerate(unique_tickers)}\n",
        "\n",
        "unique_sectors = list(set(train_data['Sector_ID'].values) | set(test_data['Sector_ID'].values))\n",
        "sector_mapping = {sector_id: sector_id for sector_id in unique_sectors}\n",
        "\n",
        "train_ticker_ids = train_data['Ticker_ID'].values\n",
        "test_ticker_ids = test_data['Ticker_ID'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mymmI-pfMLbm",
        "outputId": "ee9a6ec1-354b-417f-8b95-81809e312303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training shapes - X: (35406, 50, 6), y: (35406,)\n",
            "Test shapes - X: (8840, 50, 6), y: (8840,)\n"
          ]
        }
      ],
      "source": [
        "# Create sequences function for sector-wise training\n",
        "def create_sequences_multi(data, ticker_ids, sector_ids, seq_length=50, target_col='Log_Returns'):\n",
        "    \"\"\"Create sequences from multiple features (without including Ticker_ID) for sector-wise training\"\"\"\n",
        "    X, y, ticker_seq_list, sector_seq_list = [], [], [], []\n",
        "    data_values = data.drop(['Ticker_ID', 'Sector_ID'], axis=1).values  # Drop 'Ticker_ID' and 'Sector_ID' from features\n",
        "\n",
        "    for sector_id in np.unique(sector_ids):\n",
        "        sector_data = data[data['Sector_ID'] == sector_id]\n",
        "        sector_ticker_ids = sector_data['Ticker_ID'].values\n",
        "\n",
        "        for ticker_id in np.unique(sector_ticker_ids):\n",
        "            ticker_data = sector_data[sector_data['Ticker_ID'] == ticker_id]\n",
        "            features = ticker_data.drop(['Log_Returns', 'Ticker_ID', 'Sector_ID'], axis=1).values\n",
        "            targets = ticker_data['Log_Returns'].values\n",
        "\n",
        "            for i in range(len(features) - seq_length):\n",
        "                X.append(features[i:i+seq_length, :])  # Selecting only the features\n",
        "                ticker_seq_list.append(ticker_id)\n",
        "                sector_seq_list.append(sector_id)  # Include the sector_id in the sequence\n",
        "                y.append(targets[i + seq_length])\n",
        "\n",
        "    return np.array(X), np.array(y), np.array(ticker_seq_list), np.array(sector_seq_list)\n",
        "\n",
        "\n",
        "# Create sequences for train and test data sector-wise\n",
        "seq_length=50\n",
        "X_train, y_train, train_tickers, train_sectors = create_sequences_multi(train_data, train_ticker_ids, train_data['Sector_ID'].values, seq_length=50)\n",
        "X_test, y_test, test_tickers, test_sectors = create_sequences_multi(test_data, test_ticker_ids, test_data['Sector_ID'].values, seq_length=50)\n",
        "\n",
        "# Check shapes\n",
        "print(f\"Training shapes - X: {X_train.shape}, y: {y_train.shape}\")\n",
        "print(f\"Test shapes - X: {X_test.shape}, y: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ROZ8S4AnMNKO"
      },
      "outputs": [],
      "source": [
        "class LSTM:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, ticker_dim, embedding_dim, sector_dim, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.lr = learning_rate\n",
        "        self.ticker_dim = ticker_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sector_dim = sector_dim  # Number of sectors\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.t = 1  # Time step for Adam\n",
        "\n",
        "        # Combined input size: hidden + input + ticker + sector embedding\n",
        "        concat_dim = hidden_dim + input_dim + embedding_dim + sector_dim\n",
        "\n",
        "        # Initialize weights\n",
        "        self.Wf = np.random.randn(hidden_dim, concat_dim) * 0.01\n",
        "        self.bf = np.zeros((hidden_dim, 1))\n",
        "\n",
        "        self.Wi = np.random.randn(hidden_dim, concat_dim) * 0.01\n",
        "        self.bi = np.zeros((hidden_dim, 1))\n",
        "\n",
        "        self.Wc = np.random.randn(hidden_dim, concat_dim) * 0.01\n",
        "        self.bc = np.zeros((hidden_dim, 1))\n",
        "\n",
        "        self.Wo = np.random.randn(hidden_dim, concat_dim) * 0.01\n",
        "        self.bo = np.zeros((hidden_dim, 1))\n",
        "\n",
        "        # Output layer\n",
        "        self.Wy = np.random.randn(output_dim, hidden_dim) * 0.01\n",
        "        self.by = np.zeros((output_dim, 1))\n",
        "\n",
        "        # Initialize embedding matrices\n",
        "        self.ticker_embedding = np.random.randn(self.ticker_dim, self.embedding_dim) * 0.01\n",
        "        self.sector_embedding = np.random.randn(self.sector_dim, self.embedding_dim) * 0.01\n",
        "\n",
        "        # Initialize Adam moment estimates\n",
        "        self._init_adam_params()\n",
        "\n",
        "    def _init_adam_params(self):\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        for param_name in ['Wf', 'Wi', 'Wc', 'Wo', 'Wy', 'bf', 'bi', 'bc', 'bo', 'by']:\n",
        "            param = getattr(self, param_name)\n",
        "            self.m[param_name] = np.zeros_like(param)\n",
        "            self.v[param_name] = np.zeros_like(param)\n",
        "\n",
        "    def get_ticker_embedding(self, ticker_id):\n",
        "        \"\"\"Return the embedding for the ticker ID\"\"\"\n",
        "        return self.ticker_embedding[ticker_id].reshape(-1, 1)\n",
        "\n",
        "    def get_sector_embedding(self, sector_id):\n",
        "        \"\"\"Return the embedding for the sector ID\"\"\"\n",
        "        return self.sector_embedding[sector_id].reshape(-1, 1)\n",
        "\n",
        "    def sigmoid(self, x): return 1 / (1 + np.exp(-x))\n",
        "    def dsigmoid(self, x): return x * (1 - x)\n",
        "    def tanh(self, x): return np.tanh(x)\n",
        "    def dtanh(self, x): return 1 - x ** 2\n",
        "\n",
        "    def forward(self, x_seq, ticker_id, sector_id, h=None, c=None):\n",
        "        if h is None:\n",
        "            h = np.zeros((self.hidden_dim, 1))\n",
        "        if c is None:\n",
        "            c = np.zeros((self.hidden_dim, 1))\n",
        "        self.caches = []\n",
        "\n",
        "        # Get the ticker and sector embeddings once per sequence (not per timestep)\n",
        "        ticker_embedding = self.get_ticker_embedding(ticker_id).reshape(-1, 1)\n",
        "        sector_embedding = self.get_sector_embedding(sector_id).reshape(-1, 1)\n",
        "\n",
        "        for x in x_seq:\n",
        "            x = x.reshape(self.input_dim, 1)\n",
        "\n",
        "            # Concatenate previous hidden state, input, ticker embedding, and sector embedding\n",
        "            concat = np.vstack((h, x, ticker_embedding, sector_embedding))\n",
        "\n",
        "            ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
        "            it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
        "            c_tilde = self.tanh(np.dot(self.Wc, concat) + self.bc)\n",
        "            c = ft * c + it * c_tilde\n",
        "            ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
        "            h = ot * self.tanh(c)\n",
        "\n",
        "            self.caches.append((h, c, ft, it, c_tilde, ot, concat))\n",
        "\n",
        "        y_hat = np.dot(self.Wy, h) + self.by\n",
        "        return y_hat, h, c\n",
        "\n",
        "    def backward(self, x_seq, y_hat, y_true):\n",
        "        dh_next = np.zeros((self.hidden_dim, 1))\n",
        "        dc_next = np.zeros((self.hidden_dim, 1))\n",
        "\n",
        "        grads = {\n",
        "            'Wf': np.zeros_like(self.Wf), 'Wi': np.zeros_like(self.Wi),\n",
        "            'Wc': np.zeros_like(self.Wc), 'Wo': np.zeros_like(self.Wo),\n",
        "            'Wy': np.zeros_like(self.Wy),\n",
        "            'bf': np.zeros_like(self.bf), 'bi': np.zeros_like(self.bi),\n",
        "            'bc': np.zeros_like(self.bc), 'bo': np.zeros_like(self.bo),\n",
        "            'by': np.zeros_like(self.by)\n",
        "        }\n",
        "\n",
        "        dy = y_hat - y_true\n",
        "        grads['Wy'] += np.dot(dy, self.caches[-1][0].T)\n",
        "        grads['by'] += dy\n",
        "\n",
        "        dh = np.dot(self.Wy.T, dy) + dh_next\n",
        "\n",
        "        for t in reversed(range(len(x_seq))):\n",
        "            h, c, ft, it, c_tilde, ot, concat = self.caches[t]\n",
        "            c_prev = self.caches[t - 1][1] if t > 0 else np.zeros_like(c)\n",
        "\n",
        "            do = dh * self.tanh(c)\n",
        "            do_raw = do * self.dsigmoid(ot)\n",
        "\n",
        "            dc = dh * ot * self.dtanh(self.tanh(c)) + dc_next\n",
        "            dc_tilde = dc * it\n",
        "            dc_tilde_raw = dc_tilde * self.dtanh(c_tilde)\n",
        "\n",
        "            di = dc * c_tilde\n",
        "            di_raw = di * self.dsigmoid(it)\n",
        "\n",
        "            df = dc * c_prev\n",
        "            df_raw = df * self.dsigmoid(ft)\n",
        "\n",
        "            grads['Wf'] += np.dot(df_raw, concat.T)\n",
        "            grads['Wi'] += np.dot(di_raw, concat.T)\n",
        "            grads['Wc'] += np.dot(dc_tilde_raw, concat.T)\n",
        "            grads['Wo'] += np.dot(do_raw, concat.T)\n",
        "\n",
        "            grads['bf'] += df_raw\n",
        "            grads['bi'] += di_raw\n",
        "            grads['bc'] += dc_tilde_raw\n",
        "            grads['bo'] += do_raw\n",
        "\n",
        "            dconcat = (np.dot(self.Wf.T, df_raw) +\n",
        "                       np.dot(self.Wi.T, di_raw) +\n",
        "                       np.dot(self.Wc.T, dc_tilde_raw) +\n",
        "                       np.dot(self.Wo.T, do_raw))\n",
        "\n",
        "            dh = dconcat[:self.hidden_dim, :]\n",
        "            dc_next = dc * ft\n",
        "\n",
        "        self._apply_adam(grads)\n",
        "        self.t += 1  # Increment timestep\n",
        "\n",
        "    def _apply_adam(self, grads):\n",
        "        for param_name in grads:\n",
        "            grad = grads[param_name]\n",
        "            self.m[param_name] = self.beta1 * self.m[param_name] + (1 - self.beta1) * grad\n",
        "            self.v[param_name] = self.beta2 * self.v[param_name] + (1 - self.beta2) * (grad ** 2)\n",
        "\n",
        "            m_hat = self.m[param_name] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[param_name] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            param = getattr(self, param_name)\n",
        "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "            setattr(self, param_name, param)\n",
        "\n",
        "\n",
        "    def train(self, X_train, y_train, ticker_ids_train, sector_ids_train, epochs=10, batch_size=32):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            indices = np.arange(len(X_train))\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                batch_indices = indices[i:i + batch_size]\n",
        "                batch_loss = 0\n",
        "                batch_grads = []\n",
        "\n",
        "                for j in batch_indices:\n",
        "                    x_seq = X_train[j]\n",
        "                    y_true = y_train[j].reshape(self.output_dim, 1)\n",
        "                    ticker_ids = ticker_ids_train[j]\n",
        "                    sector_ids = sector_ids_train[j]\n",
        "                    y_hat, _, _ = self.forward(x_seq, ticker_ids, sector_ids)\n",
        "                    loss = np.mean((y_hat - y_true) ** 2)\n",
        "                    batch_loss += loss\n",
        "\n",
        "                    self.backward(x_seq, y_hat, y_true)\n",
        "\n",
        "                total_loss += batch_loss\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.6f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the sector to ID mapping\n",
        "print(sector_to_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyL808Nqh9Dx",
        "outputId": "3b454e06-ef8c-45d3-9ea2-b2c511aaa828"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Technology': 0, 'Healthcare': 1, 'Financial Services': 2, 'Physical Assets & Resources': 3, 'Consumer Cyclical': 4, 'Industrials': 5, 'Communication Services': 6, 'Consumer Defensive': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sector_mapping = {\n",
        "    0: \"Technology\",\n",
        "    1: \"Healthcare\",\n",
        "    2: \"Financial Services\",\n",
        "    3: \"Physical Assets & Resources\",\n",
        "    4: \"Consumer Cyclical\",\n",
        "    5: \"Industrials\",\n",
        "    6: \"Communication Services\",\n",
        "    7: \"Consumer Defensive\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "0pZtwcDL85wa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id, params, sector_models=None):\n",
        "    if sector_models is None:\n",
        "        sector_models = {}\n",
        "\n",
        "    # Select data for the given sector\n",
        "    sector_indices = np.where(train_sectors == sector_id)[0]\n",
        "\n",
        "    if len(sector_indices) == 0:\n",
        "        print(f\"No training data for sector {sector_id}. Skipping.\")\n",
        "        return sector_models\n",
        "\n",
        "    X_sector = X_train[sector_indices]\n",
        "    y_sector = y_train[sector_indices]\n",
        "    ticker_sector = train_tickers[sector_indices]\n",
        "    sector_sector = train_sectors[sector_indices]\n",
        "\n",
        "    # Get sector name for printing\n",
        "    sector_name = sector_mapping.get(sector_id, f\"Sector {sector_id}\")\n",
        "\n",
        "    # Check if model for sector already exists (for fine-tuning)\n",
        "    if sector_id in sector_models:\n",
        "        model = sector_models[sector_id]\n",
        "        print(f\"\\nFine-tuning sector {sector_name}...\")\n",
        "    else:\n",
        "        model = LSTM(\n",
        "            input_dim=params['input_dim'],\n",
        "            hidden_dim=params['hidden_dim'],\n",
        "            output_dim=params['output_dim'],\n",
        "            ticker_dim=params['ticker_dim'],\n",
        "            embedding_dim=params['embedding_dim'],\n",
        "            sector_dim=params['sector_dim'],\n",
        "            learning_rate=params['learning_rate']\n",
        "        )\n",
        "        print(f\"\\nTraining {sector_name} sector from scratch...\")\n",
        "\n",
        "    # Train\n",
        "    model.train(X_sector, y_sector, ticker_sector, sector_sector, epochs=params['epochs'], batch_size=params['batch_size'])\n",
        "\n",
        "    # Save the model\n",
        "    sector_models[sector_id] = model\n",
        "\n",
        "    return sector_models\n"
      ],
      "metadata": {
        "id": "OH_jHCLGyn9j"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty models\n",
        "sector_models = {}"
      ],
      "metadata": {
        "id": "kuAVTvcgBfsQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mj3rHm-IwEBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- TECHNOLOGY -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "tpgmAyPUwDFg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Technology sector (sector_id = 0)\n",
        "technology_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 0]\n",
        "\n",
        "# Get stock names correctly\n",
        "technology_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(technology_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Technology sector:\")\n",
        "print(technology_stock_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoKh99asf0xC",
        "outputId": "41700b81-bc37-4995-c210-6cab2e5b799d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stocks being trained in Technology sector:\n",
            "['AAPL', 'MSFT', 'ACN', 'ADBE', 'NVDA', 'ORCL', 'AVGO', 'IBM', 'INTC', 'INTU', 'QCOM', 'CRM', 'CSCO']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_technology = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 6,\n",
        "    'batch_size': 32\n",
        "}\n",
        "\n",
        "# Technology (sector_id -> 0)\n",
        "# sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id=0, params=params_technology, sector_models=sector_models)"
      ],
      "metadata": {
        "id": "v7dJ1zr1Q3V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "Bl9LOkpPahrh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "uSe6B4AhahoJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "bTHZS0lRahlp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "def walk_forward_validation(data_df, sector_id, params, start_date, end_date, train_window_days=365, test_window_days=30):\n",
        "    \"\"\"\n",
        "    Perform walk-forward validation on sector-specific data.\n",
        "\n",
        "    Parameters:\n",
        "        data_df: DataFrame with columns ['Date', 'X', 'y', 'ticker_id', 'sector_id']\n",
        "        sector_id: Sector ID (e.g., 0 for Technology)\n",
        "        params: Model parameters dictionary\n",
        "        start_date: Start of the walk-forward validation window (string or datetime)\n",
        "        end_date: End of the walk-forward validation window (string or datetime)\n",
        "        train_window_days: Size of training window in days\n",
        "        test_window_days: Size of testing window in days\n",
        "    \"\"\"\n",
        "    # Convert dates\n",
        "    start = pd.to_datetime(start_date)\n",
        "    end = pd.to_datetime(end_date)\n",
        "\n",
        "    current_date = start\n",
        "    walk_id = 0\n",
        "\n",
        "    while current_date + timedelta(days=train_window_days + test_window_days) <= end:\n",
        "        train_start = current_date\n",
        "        train_end = train_start + timedelta(days=train_window_days)\n",
        "        test_start = train_end\n",
        "        test_end = test_start + timedelta(days=test_window_days)\n",
        "\n",
        "        print(f\"\\n=== Walk {walk_id}: {train_start.date()} to {test_end.date()} ===\")\n",
        "\n",
        "        # Filter training data\n",
        "        train_data = data_df[(data_df['Date'] >= train_start) & (data_df['Date'] < train_end) & (data_df['sector_id'] == sector_id)]\n",
        "        test_data = data_df[(data_df['Date'] >= test_start) & (data_df['Date'] < test_end) & (data_df['sector_id'] == sector_id)]\n",
        "\n",
        "        if len(train_data) == 0 or len(test_data) == 0:\n",
        "            print(\"Skipping due to lack of data.\")\n",
        "            current_date += timedelta(days=test_window_days)\n",
        "            continue\n",
        "\n",
        "        # Convert to arrays\n",
        "        X_train = np.stack(train_data['X'].values)\n",
        "        y_train = np.stack(train_data['y'].values)\n",
        "        train_tickers = train_data['ticker_id'].values\n",
        "        train_sectors = train_data['sector_id'].values\n",
        "\n",
        "        X_test = np.stack(test_data['X'].values)\n",
        "        y_test = np.stack(test_data['y'].values)\n",
        "        test_tickers = test_data['ticker_id'].values\n",
        "\n",
        "        # Train model on current window\n",
        "        sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id, params)\n",
        "\n",
        "        # Predict\n",
        "        log_returns = predict_logreturns(sector_models[sector_id], X_test, test_tickers, sector_id, scalers)\n",
        "\n",
        "        # Volatility\n",
        "        volatility = calculate_volatility(log_returns)\n",
        "\n",
        "        # Save predictions\n",
        "        output_filename = f'technology_sector_predictions_walk{walk_id}.csv'\n",
        "        save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping)\n",
        "\n",
        "        # Advance window\n",
        "        current_date += timedelta(days=test_window_days)\n",
        "        walk_id += 1\n"
      ],
      "metadata": {
        "id": "_AdUmUAqJUm-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract start and end dates from your original dataset (df)\n",
        "start_date = df['Date'].min()\n",
        "end_date = df['Date'].max()\n",
        "\n",
        "# Create the combined DataFrame once lengths are confirmed to match\n",
        "combined_df = pd.DataFrame({\n",
        "    'Date': df['Date'].iloc[:len(X_train)],  # Make sure we slice df['Date'] to match X_train length\n",
        "    'X': list(X_train),                     # X_train: 2D sequences\n",
        "    'y': y_train.flatten(),                 # y_train: Flattened target values\n",
        "    'ticker_id': train_tickers,             # Assuming train_tickers holds the ticker IDs\n",
        "    'sector_id': train_sectors              # Assuming train_sectors holds the sector IDs\n",
        "})\n",
        "\n",
        "# Optional: Sort by date for chronological order\n",
        "combined_df = combined_df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Preview the combined_df\n",
        "print(combined_df.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABhjF7tMOCP5",
        "outputId": "c352b3fe-f7a3-48de-9720-5df0a8c1edfd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date                                                  X         y  \\\n",
            "0  2023-01-03  [[0.021892605168306947, 0.030994682917296866, ...  0.614997   \n",
            "1  2023-01-03  [[0.03383293152758216, 0.04147293256184843, 0....  0.579069   \n",
            "2  2023-01-03  [[0.052016600173119154, 0.056808307454200284, ...  0.575439   \n",
            "3  2023-01-03  [[0.01859204511910545, 0.02327759158298365, 0....  0.577445   \n",
            "4  2023-01-03  [[0.11329244047522373, 0.13836609063012026, 0....  0.578943   \n",
            "5  2023-01-03  [[0.03375618540322741, 0.046241754179337714, 0...  0.638673   \n",
            "6  2023-01-03  [[0.05128331701461588, 0.055935518453863776, 0...  0.604903   \n",
            "7  2023-01-03  [[0.018355014465572944, 0.018691514041332104, ...  0.578783   \n",
            "8  2023-01-03  [[0.004263860925549672, 0.006922971408802896, ...  0.508508   \n",
            "9  2023-01-03  [[0.047765557112874606, 0.05426056430681883, 0...  0.563300   \n",
            "10 2023-01-03  [[0.051547167208593536, 0.06157209873192007, 0...  0.552033   \n",
            "11 2023-01-03  [[0.033180583692361314, 0.035437199651026846, ...  0.593139   \n",
            "12 2023-01-03  [[0.03391649882343224, 0.034185974056011516, 0...  0.581920   \n",
            "13 2023-01-03  [[0.08358784784333002, 0.09660991824620162, 0....  0.584154   \n",
            "14 2023-01-03  [[0.01316269163096403, 0.017091027272499545, 0...  0.578343   \n",
            "15 2023-01-03  [[0.016086644141120127, 0.01894996229095674, 0...  0.558891   \n",
            "16 2023-01-03  [[0.007622384320120791, 0.009231956496725532, ...  0.567151   \n",
            "17 2023-01-03  [[0.005057690287244165, 0.005337638822017319, ...  0.591039   \n",
            "18 2023-01-03  [[0.04464095786414843, 0.05795048803840694, 0....  0.655798   \n",
            "19 2023-01-03  [[0.03289583084028597, 0.035652975764457855, 0...  0.610521   \n",
            "\n",
            "    ticker_id  sector_id  \n",
            "0           0          0  \n",
            "1          55          4  \n",
            "2          57          4  \n",
            "3          69          4  \n",
            "4          47          0  \n",
            "5          11          5  \n",
            "6          18          5  \n",
            "7          33          5  \n",
            "8          46          0  \n",
            "9          36          5  \n",
            "10         37          5  \n",
            "11         38          5  \n",
            "12         45          0  \n",
            "13         54          5  \n",
            "14         62          5  \n",
            "15         78          5  \n",
            "16         26          0  \n",
            "17         21          6  \n",
            "18          3          0  \n",
            "19         72          7  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform walk-forward validation using the start and end dates from your data\n",
        "walk_forward_validation(\n",
        "    data_df=combined_df,\n",
        "    sector_id=0,  #Technology sector\n",
        "    params=params_technology,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date,\n",
        "    train_window_days=365,\n",
        "    test_window_days=30\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgT3MQh9NlnM",
        "outputId": "08713362-288b-461f-b476-fee260d4b34d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Walk 0: 2023-01-03 to 2024-02-02 ===\n",
            "\n",
            "Training Technology sector from scratch...\n",
            "Epoch 1/6, Loss: 14.888050\n",
            "Epoch 2/6, Loss: 6.332429\n",
            "Epoch 3/6, Loss: 5.748539\n",
            "Epoch 4/6, Loss: 5.596195\n",
            "Epoch 5/6, Loss: 5.456545\n",
            "Epoch 6/6, Loss: 5.386955\n",
            "Results for sector 'Technology' saved to technology_sector_predictions_walk0.csv\n",
            "\n",
            "=== Walk 1: 2023-02-02 to 2024-03-03 ===\n",
            "\n",
            "Training Technology sector from scratch...\n",
            "Epoch 1/6, Loss: 16.452685\n",
            "Epoch 2/6, Loss: 6.194816\n",
            "Epoch 3/6, Loss: 5.876626\n",
            "Epoch 4/6, Loss: 5.713865\n",
            "Epoch 5/6, Loss: 5.546906\n",
            "Epoch 6/6, Loss: 5.386111\n",
            "Results for sector 'Technology' saved to technology_sector_predictions_walk1.csv\n",
            "\n",
            "=== Walk 2: 2023-03-04 to 2024-04-02 ===\n",
            "\n",
            "Training Technology sector from scratch...\n",
            "Epoch 1/6, Loss: 19.212191\n",
            "Epoch 2/6, Loss: 6.505210\n",
            "Epoch 3/6, Loss: 6.104164\n",
            "Epoch 4/6, Loss: 5.908045\n",
            "Epoch 5/6, Loss: 5.712412\n",
            "Epoch 6/6, Loss: 5.570509\n",
            "Results for sector 'Technology' saved to technology_sector_predictions_walk2.csv\n",
            "\n",
            "=== Walk 3: 2023-04-03 to 2024-05-02 ===\n",
            "\n",
            "Training Technology sector from scratch...\n",
            "Epoch 1/6, Loss: 15.556696\n",
            "Epoch 2/6, Loss: 6.303621\n",
            "Epoch 3/6, Loss: 6.002240\n",
            "Epoch 4/6, Loss: 5.724058\n",
            "Epoch 5/6, Loss: 5.717907\n",
            "Epoch 6/6, Loss: 5.558764\n",
            "Results for sector 'Technology' saved to technology_sector_predictions_walk3.csv\n",
            "\n",
            "=== Walk 4: 2023-05-03 to 2024-06-01 ===\n",
            "\n",
            "Training Technology sector from scratch...\n",
            "Epoch 1/6, Loss: 15.276125\n",
            "Epoch 2/6, Loss: 6.288730\n",
            "Epoch 3/6, Loss: 6.103492\n",
            "Epoch 4/6, Loss: 5.670840\n",
            "Epoch 5/6, Loss: 5.574912\n",
            "Epoch 6/6, Loss: 5.478137\n",
            "Results for sector 'Technology' saved to technology_sector_predictions_walk4.csv\n",
            "\n",
            "=== Walk 5: 2023-06-02 to 2024-07-01 ===\n",
            "\n",
            "Training Technology sector from scratch...\n",
            "Epoch 1/6, Loss: 14.546011\n",
            "Epoch 2/6, Loss: 6.394744\n",
            "Epoch 3/6, Loss: 5.950070\n",
            "Epoch 4/6, Loss: 5.681341\n",
            "Epoch 5/6, Loss: 5.526939\n",
            "Epoch 6/6, Loss: 5.457347\n",
            "Results for sector 'Technology' saved to technology_sector_predictions_walk5.csv\n",
            "\n",
            "=== Walk 6: 2023-07-02 to 2024-07-31 ===\n",
            "\n",
            "Training Technology sector from scratch...\n",
            "Epoch 1/6, Loss: 15.889403\n",
            "Epoch 2/6, Loss: 6.318129\n",
            "Epoch 3/6, Loss: 5.914510\n",
            "Epoch 4/6, Loss: 5.766815\n",
            "Epoch 5/6, Loss: 5.531537\n",
            "Epoch 6/6, Loss: 5.552183\n",
            "Results for sector 'Technology' saved to technology_sector_predictions_walk6.csv\n",
            "\n",
            "=== Walk 7: 2023-08-01 to 2024-08-30 ===\n",
            "\n",
            "Training Technology sector from scratch...\n",
            "Epoch 1/6, Loss: 18.480071\n",
            "Epoch 2/6, Loss: 6.223575\n",
            "Epoch 3/6, Loss: 5.823569\n",
            "Epoch 4/6, Loss: 5.601547\n",
            "Epoch 5/6, Loss: 5.388613\n",
            "Epoch 6/6, Loss: 5.243916\n",
            "Results for sector 'Technology' saved to technology_sector_predictions_walk7.csv\n",
            "\n",
            "=== Walk 8: 2023-08-31 to 2024-09-29 ===\n",
            "\n",
            "Training Technology sector from scratch...\n",
            "Epoch 1/6, Loss: 15.129876\n",
            "Epoch 2/6, Loss: 5.971060\n",
            "Epoch 3/6, Loss: 5.756268\n",
            "Epoch 4/6, Loss: 5.584073\n",
            "Epoch 5/6, Loss: 5.443653\n",
            "Epoch 6/6, Loss: 5.373001\n",
            "Results for sector 'Technology' saved to technology_sector_predictions_walk8.csv\n",
            "\n",
            "=== Walk 9: 2023-09-30 to 2024-10-29 ===\n",
            "\n",
            "Training Technology sector from scratch...\n",
            "Epoch 1/6, Loss: 17.302455\n",
            "Epoch 2/6, Loss: 6.271165\n",
            "Epoch 3/6, Loss: 5.635380\n",
            "Epoch 4/6, Loss: 5.587783\n",
            "Epoch 5/6, Loss: 5.387245\n",
            "Epoch 6/6, Loss: 5.399451\n",
            "Results for sector 'Technology' saved to technology_sector_predictions_walk9.csv\n",
            "\n",
            "=== Walk 10: 2023-10-30 to 2024-11-28 ===\n",
            "\n",
            "Training Technology sector from scratch...\n",
            "Epoch 1/6, Loss: 15.109712\n",
            "Epoch 2/6, Loss: 5.899156\n",
            "Epoch 3/6, Loss: 5.505420\n",
            "Epoch 4/6, Loss: 5.327915\n",
            "Epoch 5/6, Loss: 5.380122\n",
            "Epoch 6/6, Loss: 5.132389\n",
            "Results for sector 'Technology' saved to technology_sector_predictions_walk10.csv\n",
            "\n",
            "=== Walk 11: 2023-11-29 to 2024-12-28 ===\n",
            "\n",
            "Training Technology sector from scratch...\n",
            "Epoch 1/6, Loss: 16.024567\n",
            "Epoch 2/6, Loss: 5.688647\n",
            "Epoch 3/6, Loss: 5.432352\n",
            "Epoch 4/6, Loss: 5.149162\n",
            "Epoch 5/6, Loss: 4.974899\n",
            "Epoch 6/6, Loss: 4.984209\n",
            "Results for sector 'Technology' saved to technology_sector_predictions_walk11.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eevbwzaSHWIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- HEALTHCARE -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "YpGN3GJPap39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Health sector\n",
        "health_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 1]\n",
        "\n",
        "# Get stock names correctly\n",
        "health_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(health_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in health sector:\")\n",
        "print(health_stock_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkvlsjWQ5wrI",
        "outputId": "8bce8ca3-fdc0-4b52-eb32-c66dea7b5ac1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stocks being trained in health sector:\n",
            "['MRK', 'ABBV', 'ABT', 'AMGN', 'GILD', 'MDT', 'PFE', 'BMY', 'JNJ', 'LLY', 'CVS', 'DHR']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_healthcare = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}"
      ],
      "metadata": {
        "id": "rtQ0nqkpCFRJ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "aU2l2Wrw6UMb"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "Ya3Hr8_W6UIH"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "d-r6tIEu6UFK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "def walk_forward_validation(data_df, sector_id, params, start_date, end_date, train_window_days=365, test_window_days=30):\n",
        "    \"\"\"\n",
        "    Perform walk-forward validation on sector-specific data.\n",
        "\n",
        "    Parameters:\n",
        "        data_df: DataFrame with columns ['Date', 'X', 'y', 'ticker_id', 'sector_id']\n",
        "        sector_id: Sector ID (e.g., 0 for Technology)\n",
        "        params: Model parameters dictionary\n",
        "        start_date: Start of the walk-forward validation window (string or datetime)\n",
        "        end_date: End of the walk-forward validation window (string or datetime)\n",
        "        train_window_days: Size of training window in days\n",
        "        test_window_days: Size of testing window in days\n",
        "    \"\"\"\n",
        "    # Convert dates\n",
        "    start = pd.to_datetime(start_date)\n",
        "    end = pd.to_datetime(end_date)\n",
        "\n",
        "    current_date = start\n",
        "    walk_id = 0\n",
        "\n",
        "    while current_date + timedelta(days=train_window_days + test_window_days) <= end:\n",
        "        train_start = current_date\n",
        "        train_end = train_start + timedelta(days=train_window_days)\n",
        "        test_start = train_end\n",
        "        test_end = test_start + timedelta(days=test_window_days)\n",
        "\n",
        "        print(f\"\\n=== Walk {walk_id}: {train_start.date()} to {test_end.date()} ===\")\n",
        "\n",
        "        # Filter training data\n",
        "        train_data = data_df[(data_df['Date'] >= train_start) & (data_df['Date'] < train_end) & (data_df['sector_id'] == sector_id)]\n",
        "        test_data = data_df[(data_df['Date'] >= test_start) & (data_df['Date'] < test_end) & (data_df['sector_id'] == sector_id)]\n",
        "\n",
        "        if len(train_data) == 0 or len(test_data) == 0:\n",
        "            print(\"Skipping due to lack of data.\")\n",
        "            current_date += timedelta(days=test_window_days)\n",
        "            continue\n",
        "\n",
        "        # Convert to arrays\n",
        "        X_train = np.stack(train_data['X'].values)\n",
        "        y_train = np.stack(train_data['y'].values)\n",
        "        train_tickers = train_data['ticker_id'].values\n",
        "        train_sectors = train_data['sector_id'].values\n",
        "\n",
        "        X_test = np.stack(test_data['X'].values)\n",
        "        y_test = np.stack(test_data['y'].values)\n",
        "        test_tickers = test_data['ticker_id'].values\n",
        "\n",
        "        # Train model on current window\n",
        "        sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id, params)\n",
        "\n",
        "        # Predict\n",
        "        log_returns = predict_logreturns(sector_models[sector_id], X_test, test_tickers, sector_id, scalers)\n",
        "\n",
        "        # Volatility\n",
        "        volatility = calculate_volatility(log_returns)\n",
        "\n",
        "        # Save predictions\n",
        "        output_filename = f'Health_sector_predictions_walk{walk_id}.csv'\n",
        "        save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping)\n",
        "\n",
        "        # Advance window\n",
        "        current_date += timedelta(days=test_window_days)\n",
        "        walk_id += 1\n"
      ],
      "metadata": {
        "id": "PbeK_Idn6UCC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract start and end dates from your original dataset (df)\n",
        "start_date = df['Date'].min()\n",
        "end_date = df['Date'].max()\n",
        "\n",
        "# Create the combined DataFrame once lengths are confirmed to match\n",
        "combined_df = pd.DataFrame({\n",
        "    'Date': df['Date'].iloc[:len(X_train)],  # Make sure we slice df['Date'] to match X_train length\n",
        "    'X': list(X_train),                     # X_train: 2D sequences\n",
        "    'y': y_train.flatten(),                 # y_train: Flattened target values\n",
        "    'ticker_id': train_tickers,             # Assuming train_tickers holds the ticker IDs\n",
        "    'sector_id': train_sectors              # Assuming train_sectors holds the sector IDs\n",
        "})\n",
        "\n",
        "# Optional: Sort by date for chronological order\n",
        "combined_df = combined_df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Preview the combined_df\n",
        "print(combined_df.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQpepfRQYPgk",
        "outputId": "cf62b5f8-4a97-443a-bbe8-3c9104b5a432"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date                                                  X         y  \\\n",
            "0  2023-01-03  [[0.021892605168306947, 0.030994682917296866, ...  0.614997   \n",
            "1  2023-01-03  [[0.03383293152758216, 0.04147293256184843, 0....  0.579069   \n",
            "2  2023-01-03  [[0.052016600173119154, 0.056808307454200284, ...  0.575439   \n",
            "3  2023-01-03  [[0.01859204511910545, 0.02327759158298365, 0....  0.577445   \n",
            "4  2023-01-03  [[0.11329244047522373, 0.13836609063012026, 0....  0.578943   \n",
            "5  2023-01-03  [[0.03375618540322741, 0.046241754179337714, 0...  0.638673   \n",
            "6  2023-01-03  [[0.05128331701461588, 0.055935518453863776, 0...  0.604903   \n",
            "7  2023-01-03  [[0.018355014465572944, 0.018691514041332104, ...  0.578783   \n",
            "8  2023-01-03  [[0.004263860925549672, 0.006922971408802896, ...  0.508508   \n",
            "9  2023-01-03  [[0.047765557112874606, 0.05426056430681883, 0...  0.563300   \n",
            "10 2023-01-03  [[0.051547167208593536, 0.06157209873192007, 0...  0.552033   \n",
            "11 2023-01-03  [[0.033180583692361314, 0.035437199651026846, ...  0.593139   \n",
            "12 2023-01-03  [[0.03391649882343224, 0.034185974056011516, 0...  0.581920   \n",
            "13 2023-01-03  [[0.08358784784333002, 0.09660991824620162, 0....  0.584154   \n",
            "14 2023-01-03  [[0.01316269163096403, 0.017091027272499545, 0...  0.578343   \n",
            "15 2023-01-03  [[0.016086644141120127, 0.01894996229095674, 0...  0.558891   \n",
            "16 2023-01-03  [[0.007622384320120791, 0.009231956496725532, ...  0.567151   \n",
            "17 2023-01-03  [[0.005057690287244165, 0.005337638822017319, ...  0.591039   \n",
            "18 2023-01-03  [[0.04464095786414843, 0.05795048803840694, 0....  0.655798   \n",
            "19 2023-01-03  [[0.03289583084028597, 0.035652975764457855, 0...  0.610521   \n",
            "\n",
            "    ticker_id  sector_id  \n",
            "0           0          0  \n",
            "1          55          4  \n",
            "2          57          4  \n",
            "3          69          4  \n",
            "4          47          0  \n",
            "5          11          5  \n",
            "6          18          5  \n",
            "7          33          5  \n",
            "8          46          0  \n",
            "9          36          5  \n",
            "10         37          5  \n",
            "11         38          5  \n",
            "12         45          0  \n",
            "13         54          5  \n",
            "14         62          5  \n",
            "15         78          5  \n",
            "16         26          0  \n",
            "17         21          6  \n",
            "18          3          0  \n",
            "19         72          7  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform walk-forward validation using the start and end dates from your data\n",
        "walk_forward_validation(\n",
        "    data_df=combined_df,\n",
        "    sector_id=1,  #Health sector\n",
        "    params=params_healthcare,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date,\n",
        "    train_window_days=365,\n",
        "    test_window_days=30\n",
        ")"
      ],
      "metadata": {
        "id": "GpDpWVQxYPdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HWAeSxcN6T8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- FINANCIAL SERVICES -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "TUC4nyPg6T43"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Financial Services sector\n",
        "finservice_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 2]\n",
        "\n",
        "# Get stock names correctly\n",
        "finservice_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(finservice_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Financial Service sector:\")\n",
        "print(finservice_stock_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_d-9lsy6T1i",
        "outputId": "14c8b43d-73e9-47ee-fa4e-7cb9af3d600a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stocks being trained in Financial Service sector:\n",
            "['MS', 'AIG', 'AXP', 'GS', 'BAC', 'BK', 'PYPL', 'BLK', 'C', 'JPM', 'COF', 'MA', 'MET']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_financial_services = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}"
      ],
      "metadata": {
        "id": "IJ8DXhJGblHe"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "SYPPxadt7yyX"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "OkxGfazc7ywO"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "D3gftc947yuE"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "def walk_forward_validation(data_df, sector_id, params, start_date, end_date, train_window_days=365, test_window_days=30):\n",
        "    \"\"\"\n",
        "    Perform walk-forward validation on sector-specific data.\n",
        "\n",
        "    Parameters:\n",
        "        data_df: DataFrame with columns ['Date', 'X', 'y', 'ticker_id', 'sector_id']\n",
        "        sector_id: Sector ID (e.g., 0 for Technology)\n",
        "        params: Model parameters dictionary\n",
        "        start_date: Start of the walk-forward validation window (string or datetime)\n",
        "        end_date: End of the walk-forward validation window (string or datetime)\n",
        "        train_window_days: Size of training window in days\n",
        "        test_window_days: Size of testing window in days\n",
        "    \"\"\"\n",
        "    # Convert dates\n",
        "    start = pd.to_datetime(start_date)\n",
        "    end = pd.to_datetime(end_date)\n",
        "\n",
        "    current_date = start\n",
        "    walk_id = 0\n",
        "\n",
        "    while current_date + timedelta(days=train_window_days + test_window_days) <= end:\n",
        "        train_start = current_date\n",
        "        train_end = train_start + timedelta(days=train_window_days)\n",
        "        test_start = train_end\n",
        "        test_end = test_start + timedelta(days=test_window_days)\n",
        "\n",
        "        print(f\"\\n=== Walk {walk_id}: {train_start.date()} to {test_end.date()} ===\")\n",
        "\n",
        "        # Filter training data\n",
        "        train_data = data_df[(data_df['Date'] >= train_start) & (data_df['Date'] < train_end) & (data_df['sector_id'] == sector_id)]\n",
        "        test_data = data_df[(data_df['Date'] >= test_start) & (data_df['Date'] < test_end) & (data_df['sector_id'] == sector_id)]\n",
        "\n",
        "        if len(train_data) == 0 or len(test_data) == 0:\n",
        "            print(\"Skipping due to lack of data.\")\n",
        "            current_date += timedelta(days=test_window_days)\n",
        "            continue\n",
        "\n",
        "        # Convert to arrays\n",
        "        X_train = np.stack(train_data['X'].values)\n",
        "        y_train = np.stack(train_data['y'].values)\n",
        "        train_tickers = train_data['ticker_id'].values\n",
        "        train_sectors = train_data['sector_id'].values\n",
        "\n",
        "        X_test = np.stack(test_data['X'].values)\n",
        "        y_test = np.stack(test_data['y'].values)\n",
        "        test_tickers = test_data['ticker_id'].values\n",
        "\n",
        "        # Train model on current window\n",
        "        sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id, params)\n",
        "\n",
        "        # Predict\n",
        "        log_returns = predict_logreturns(sector_models[sector_id], X_test, test_tickers, sector_id, scalers)\n",
        "\n",
        "        # Volatility\n",
        "        volatility = calculate_volatility(log_returns)\n",
        "\n",
        "        # Save predictions\n",
        "        output_filename = f'FinancialService_sector_predictions_walk{walk_id}.csv'\n",
        "        save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping)\n",
        "\n",
        "        # Advance window\n",
        "        current_date += timedelta(days=test_window_days)\n",
        "        walk_id += 1\n"
      ],
      "metadata": {
        "id": "sXvdSQFe7yre"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract start and end dates from your original dataset (df)\n",
        "start_date = df['Date'].min()\n",
        "end_date = df['Date'].max()\n",
        "\n",
        "# Create the combined DataFrame once lengths are confirmed to match\n",
        "combined_df = pd.DataFrame({\n",
        "    'Date': df['Date'].iloc[:len(X_train)],  # Make sure we slice df['Date'] to match X_train length\n",
        "    'X': list(X_train),                     # X_train: 2D sequences\n",
        "    'y': y_train.flatten(),                 # y_train: Flattened target values\n",
        "    'ticker_id': train_tickers,             # Assuming train_tickers holds the ticker IDs\n",
        "    'sector_id': train_sectors              # Assuming train_sectors holds the sector IDs\n",
        "})\n",
        "\n",
        "# Optional: Sort by date for chronological order\n",
        "combined_df = combined_df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Preview the combined_df\n",
        "print(combined_df.head(20))\n"
      ],
      "metadata": {
        "id": "e5-MV61e7ypa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59cdde54-0fa0-442b-b4b4-ab1a9ca478b6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date                                                  X         y  \\\n",
            "0  2023-01-03  [[0.021892605168306947, 0.030994682917296866, ...  0.614997   \n",
            "1  2023-01-03  [[0.03383293152758216, 0.04147293256184843, 0....  0.579069   \n",
            "2  2023-01-03  [[0.052016600173119154, 0.056808307454200284, ...  0.575439   \n",
            "3  2023-01-03  [[0.01859204511910545, 0.02327759158298365, 0....  0.577445   \n",
            "4  2023-01-03  [[0.11329244047522373, 0.13836609063012026, 0....  0.578943   \n",
            "5  2023-01-03  [[0.03375618540322741, 0.046241754179337714, 0...  0.638673   \n",
            "6  2023-01-03  [[0.05128331701461588, 0.055935518453863776, 0...  0.604903   \n",
            "7  2023-01-03  [[0.018355014465572944, 0.018691514041332104, ...  0.578783   \n",
            "8  2023-01-03  [[0.004263860925549672, 0.006922971408802896, ...  0.508508   \n",
            "9  2023-01-03  [[0.047765557112874606, 0.05426056430681883, 0...  0.563300   \n",
            "10 2023-01-03  [[0.051547167208593536, 0.06157209873192007, 0...  0.552033   \n",
            "11 2023-01-03  [[0.033180583692361314, 0.035437199651026846, ...  0.593139   \n",
            "12 2023-01-03  [[0.03391649882343224, 0.034185974056011516, 0...  0.581920   \n",
            "13 2023-01-03  [[0.08358784784333002, 0.09660991824620162, 0....  0.584154   \n",
            "14 2023-01-03  [[0.01316269163096403, 0.017091027272499545, 0...  0.578343   \n",
            "15 2023-01-03  [[0.016086644141120127, 0.01894996229095674, 0...  0.558891   \n",
            "16 2023-01-03  [[0.007622384320120791, 0.009231956496725532, ...  0.567151   \n",
            "17 2023-01-03  [[0.005057690287244165, 0.005337638822017319, ...  0.591039   \n",
            "18 2023-01-03  [[0.04464095786414843, 0.05795048803840694, 0....  0.655798   \n",
            "19 2023-01-03  [[0.03289583084028597, 0.035652975764457855, 0...  0.610521   \n",
            "\n",
            "    ticker_id  sector_id  \n",
            "0           0          0  \n",
            "1          55          4  \n",
            "2          57          4  \n",
            "3          69          4  \n",
            "4          47          0  \n",
            "5          11          5  \n",
            "6          18          5  \n",
            "7          33          5  \n",
            "8          46          0  \n",
            "9          36          5  \n",
            "10         37          5  \n",
            "11         38          5  \n",
            "12         45          0  \n",
            "13         54          5  \n",
            "14         62          5  \n",
            "15         78          5  \n",
            "16         26          0  \n",
            "17         21          6  \n",
            "18          3          0  \n",
            "19         72          7  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform walk-forward validation using the start and end dates from your data\n",
        "walk_forward_validation(\n",
        "    data_df=combined_df,\n",
        "    sector_id=2,  # Financial Service Sector\n",
        "    params=params_financial_services,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date,\n",
        "    train_window_days=365,\n",
        "    test_window_days=30\n",
        ")"
      ],
      "metadata": {
        "id": "XEDIPPPCYyXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8A5IRVyYySP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- PHYSICAL ASSETS -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "_0Hsj2_J7ynH"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Physical Assets & Resources sector\n",
        "phyassets_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 3]\n",
        "\n",
        "# Get stock names correctly\n",
        "phyassets_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(phyassets_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Physical Assets sector:\")\n",
        "print(phyassets_stock_names)\n"
      ],
      "metadata": {
        "id": "blMESR737yk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d18cf95-fd3c-4c8f-cce8-31ded40c8740"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stocks being trained in Physical Assets sector:\n",
            "['DUK', 'EXC', 'NEE', 'AMT', 'LIN', 'COP', 'CVX', 'DOW']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_physical_assets_resources = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}"
      ],
      "metadata": {
        "id": "hlsvnUMZCFEs"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "gm4LVEcA82Co"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "6CLev4oq81_e"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "o83q3IW38156"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "def walk_forward_validation(data_df, sector_id, params, start_date, end_date, train_window_days=365, test_window_days=30):\n",
        "    \"\"\"\n",
        "    Perform walk-forward validation on sector-specific data.\n",
        "\n",
        "    Parameters:\n",
        "        data_df: DataFrame with columns ['Date', 'X', 'y', 'ticker_id', 'sector_id']\n",
        "        sector_id: Sector ID (e.g., 0 for Technology)\n",
        "        params: Model parameters dictionary\n",
        "        start_date: Start of the walk-forward validation window (string or datetime)\n",
        "        end_date: End of the walk-forward validation window (string or datetime)\n",
        "        train_window_days: Size of training window in days\n",
        "        test_window_days: Size of testing window in days\n",
        "    \"\"\"\n",
        "    # Convert dates\n",
        "    start = pd.to_datetime(start_date)\n",
        "    end = pd.to_datetime(end_date)\n",
        "\n",
        "    current_date = start\n",
        "    walk_id = 0\n",
        "\n",
        "    while current_date + timedelta(days=train_window_days + test_window_days) <= end:\n",
        "        train_start = current_date\n",
        "        train_end = train_start + timedelta(days=train_window_days)\n",
        "        test_start = train_end\n",
        "        test_end = test_start + timedelta(days=test_window_days)\n",
        "\n",
        "        print(f\"\\n=== Walk {walk_id}: {train_start.date()} to {test_end.date()} ===\")\n",
        "\n",
        "        # Filter training data\n",
        "        train_data = data_df[(data_df['Date'] >= train_start) & (data_df['Date'] < train_end) & (data_df['sector_id'] == sector_id)]\n",
        "        test_data = data_df[(data_df['Date'] >= test_start) & (data_df['Date'] < test_end) & (data_df['sector_id'] == sector_id)]\n",
        "\n",
        "        if len(train_data) == 0 or len(test_data) == 0:\n",
        "            print(\"Skipping due to lack of data.\")\n",
        "            current_date += timedelta(days=test_window_days)\n",
        "            continue\n",
        "\n",
        "        # Convert to arrays\n",
        "        X_train = np.stack(train_data['X'].values)\n",
        "        y_train = np.stack(train_data['y'].values)\n",
        "        train_tickers = train_data['ticker_id'].values\n",
        "        train_sectors = train_data['sector_id'].values\n",
        "\n",
        "        X_test = np.stack(test_data['X'].values)\n",
        "        y_test = np.stack(test_data['y'].values)\n",
        "        test_tickers = test_data['ticker_id'].values\n",
        "\n",
        "        # Train model on current window\n",
        "        sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id, params)\n",
        "\n",
        "        # Predict\n",
        "        log_returns = predict_logreturns(sector_models[sector_id], X_test, test_tickers, sector_id, scalers)\n",
        "\n",
        "        # Volatility\n",
        "        volatility = calculate_volatility(log_returns)\n",
        "\n",
        "        # Save predictions\n",
        "        output_filename = f'PhysicalAssets_sector_predictions_walk{walk_id}.csv'\n",
        "        save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping)\n",
        "\n",
        "        # Advance window\n",
        "        current_date += timedelta(days=test_window_days)\n",
        "        walk_id += 1\n"
      ],
      "metadata": {
        "id": "18SRs4nhZMyD"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract start and end dates from your original dataset (df)\n",
        "start_date = df['Date'].min()\n",
        "end_date = df['Date'].max()\n",
        "\n",
        "# Create the combined DataFrame once lengths are confirmed to match\n",
        "combined_df = pd.DataFrame({\n",
        "    'Date': df['Date'].iloc[:len(X_train)],  # Make sure we slice df['Date'] to match X_train length\n",
        "    'X': list(X_train),                     # X_train: 2D sequences\n",
        "    'y': y_train.flatten(),                 # y_train: Flattened target values\n",
        "    'ticker_id': train_tickers,             # Assuming train_tickers holds the ticker IDs\n",
        "    'sector_id': train_sectors              # Assuming train_sectors holds the sector IDs\n",
        "})\n",
        "\n",
        "# Optional: Sort by date for chronological order\n",
        "combined_df = combined_df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Preview the combined_df\n",
        "print(combined_df.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xlJGZiFZMrr",
        "outputId": "66e2e7c0-fb9e-410f-f4a9-5cfb7698bbdb"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date                                                  X         y  \\\n",
            "0  2023-01-03  [[0.021892605168306947, 0.030994682917296866, ...  0.614997   \n",
            "1  2023-01-03  [[0.03383293152758216, 0.04147293256184843, 0....  0.579069   \n",
            "2  2023-01-03  [[0.052016600173119154, 0.056808307454200284, ...  0.575439   \n",
            "3  2023-01-03  [[0.01859204511910545, 0.02327759158298365, 0....  0.577445   \n",
            "4  2023-01-03  [[0.11329244047522373, 0.13836609063012026, 0....  0.578943   \n",
            "5  2023-01-03  [[0.03375618540322741, 0.046241754179337714, 0...  0.638673   \n",
            "6  2023-01-03  [[0.05128331701461588, 0.055935518453863776, 0...  0.604903   \n",
            "7  2023-01-03  [[0.018355014465572944, 0.018691514041332104, ...  0.578783   \n",
            "8  2023-01-03  [[0.004263860925549672, 0.006922971408802896, ...  0.508508   \n",
            "9  2023-01-03  [[0.047765557112874606, 0.05426056430681883, 0...  0.563300   \n",
            "10 2023-01-03  [[0.051547167208593536, 0.06157209873192007, 0...  0.552033   \n",
            "11 2023-01-03  [[0.033180583692361314, 0.035437199651026846, ...  0.593139   \n",
            "12 2023-01-03  [[0.03391649882343224, 0.034185974056011516, 0...  0.581920   \n",
            "13 2023-01-03  [[0.08358784784333002, 0.09660991824620162, 0....  0.584154   \n",
            "14 2023-01-03  [[0.01316269163096403, 0.017091027272499545, 0...  0.578343   \n",
            "15 2023-01-03  [[0.016086644141120127, 0.01894996229095674, 0...  0.558891   \n",
            "16 2023-01-03  [[0.007622384320120791, 0.009231956496725532, ...  0.567151   \n",
            "17 2023-01-03  [[0.005057690287244165, 0.005337638822017319, ...  0.591039   \n",
            "18 2023-01-03  [[0.04464095786414843, 0.05795048803840694, 0....  0.655798   \n",
            "19 2023-01-03  [[0.03289583084028597, 0.035652975764457855, 0...  0.610521   \n",
            "\n",
            "    ticker_id  sector_id  \n",
            "0           0          0  \n",
            "1          55          4  \n",
            "2          57          4  \n",
            "3          69          4  \n",
            "4          47          0  \n",
            "5          11          5  \n",
            "6          18          5  \n",
            "7          33          5  \n",
            "8          46          0  \n",
            "9          36          5  \n",
            "10         37          5  \n",
            "11         38          5  \n",
            "12         45          0  \n",
            "13         54          5  \n",
            "14         62          5  \n",
            "15         78          5  \n",
            "16         26          0  \n",
            "17         21          6  \n",
            "18          3          0  \n",
            "19         72          7  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform walk-forward validation using the start and end dates from your data\n",
        "walk_forward_validation(\n",
        "    data_df=combined_df,\n",
        "    sector_id=3,  # Physical Assets Sector\n",
        "    params=params_physical_assets_resources,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date,\n",
        "    train_window_days=365,\n",
        "    test_window_days=30\n",
        ")"
      ],
      "metadata": {
        "id": "V3IgempMZMjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mCrKII9C81xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- CONSUMER CYCLIC -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "Oo9pllkb81qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Consumer Cyclical sector\n",
        "concyclic_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 4]\n",
        "\n",
        "# Get stock names correctly\n",
        "concyclic_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(concyclic_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Consumer Cyclic sector:\")\n",
        "print(concyclic_stock_names)\n"
      ],
      "metadata": {
        "id": "36SisDOL81ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_consumer_cyclical = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}"
      ],
      "metadata": {
        "id": "oc4gsheyCE-8"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "c4VEfldC-E44"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "Je153DE2-EzN"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "STBEYmto-Etk"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "def walk_forward_validation(data_df, sector_id, params, start_date, end_date, train_window_days=365, test_window_days=30):\n",
        "    \"\"\"\n",
        "    Perform walk-forward validation on sector-specific data.\n",
        "\n",
        "    Parameters:\n",
        "        data_df: DataFrame with columns ['Date', 'X', 'y', 'ticker_id', 'sector_id']\n",
        "        sector_id: Sector ID (e.g., 0 for Technology)\n",
        "        params: Model parameters dictionary\n",
        "        start_date: Start of the walk-forward validation window (string or datetime)\n",
        "        end_date: End of the walk-forward validation window (string or datetime)\n",
        "        train_window_days: Size of training window in days\n",
        "        test_window_days: Size of testing window in days\n",
        "    \"\"\"\n",
        "    # Convert dates\n",
        "    start = pd.to_datetime(start_date)\n",
        "    end = pd.to_datetime(end_date)\n",
        "\n",
        "    current_date = start\n",
        "    walk_id = 0\n",
        "\n",
        "    while current_date + timedelta(days=train_window_days + test_window_days) <= end:\n",
        "        train_start = current_date\n",
        "        train_end = train_start + timedelta(days=train_window_days)\n",
        "        test_start = train_end\n",
        "        test_end = test_start + timedelta(days=test_window_days)\n",
        "\n",
        "        print(f\"\\n=== Walk {walk_id}: {train_start.date()} to {test_end.date()} ===\")\n",
        "\n",
        "        # Filter training data\n",
        "        train_data = data_df[(data_df['Date'] >= train_start) & (data_df['Date'] < train_end) & (data_df['sector_id'] == sector_id)]\n",
        "        test_data = data_df[(data_df['Date'] >= test_start) & (data_df['Date'] < test_end) & (data_df['sector_id'] == sector_id)]\n",
        "\n",
        "        if len(train_data) == 0 or len(test_data) == 0:\n",
        "            print(\"Skipping due to lack of data.\")\n",
        "            current_date += timedelta(days=test_window_days)\n",
        "            continue\n",
        "\n",
        "        # Convert to arrays\n",
        "        X_train = np.stack(train_data['X'].values)\n",
        "        y_train = np.stack(train_data['y'].values)\n",
        "        train_tickers = train_data['ticker_id'].values\n",
        "        train_sectors = train_data['sector_id'].values\n",
        "\n",
        "        X_test = np.stack(test_data['X'].values)\n",
        "        y_test = np.stack(test_data['y'].values)\n",
        "        test_tickers = test_data['ticker_id'].values\n",
        "\n",
        "        # Train model on current window\n",
        "        sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id, params)\n",
        "\n",
        "        # Predict\n",
        "        log_returns = predict_logreturns(sector_models[sector_id], X_test, test_tickers, sector_id, scalers)\n",
        "\n",
        "        # Volatility\n",
        "        volatility = calculate_volatility(log_returns)\n",
        "\n",
        "        # Save predictions\n",
        "        output_filename = f'ConsumerCyclic_sector_predictions_walk{walk_id}.csv'\n",
        "        save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping)\n",
        "\n",
        "        # Advance window\n",
        "        current_date += timedelta(days=test_window_days)\n",
        "        walk_id += 1\n"
      ],
      "metadata": {
        "id": "6VWlnOYUZqOh"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract start and end dates from your original dataset (df)\n",
        "start_date = df['Date'].min()\n",
        "end_date = df['Date'].max()\n",
        "\n",
        "# Create the combined DataFrame once lengths are confirmed to match\n",
        "combined_df = pd.DataFrame({\n",
        "    'Date': df['Date'].iloc[:len(X_train)],  # Make sure we slice df['Date'] to match X_train length\n",
        "    'X': list(X_train),                     # X_train: 2D sequences\n",
        "    'y': y_train.flatten(),                 # y_train: Flattened target values\n",
        "    'ticker_id': train_tickers,             # Assuming train_tickers holds the ticker IDs\n",
        "    'sector_id': train_sectors              # Assuming train_sectors holds the sector IDs\n",
        "})\n",
        "\n",
        "# Optional: Sort by date for chronological order\n",
        "combined_df = combined_df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Preview the combined_df\n",
        "print(combined_df.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXHQXN2-ZqK7",
        "outputId": "5abcd4f1-a2ba-42bc-d09e-97087c7d9f0d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date                                                  X         y  \\\n",
            "0  2023-01-03  [[0.021892605168306947, 0.030994682917296866, ...  0.614997   \n",
            "1  2023-01-03  [[0.03383293152758216, 0.04147293256184843, 0....  0.579069   \n",
            "2  2023-01-03  [[0.052016600173119154, 0.056808307454200284, ...  0.575439   \n",
            "3  2023-01-03  [[0.01859204511910545, 0.02327759158298365, 0....  0.577445   \n",
            "4  2023-01-03  [[0.11329244047522373, 0.13836609063012026, 0....  0.578943   \n",
            "5  2023-01-03  [[0.03375618540322741, 0.046241754179337714, 0...  0.638673   \n",
            "6  2023-01-03  [[0.05128331701461588, 0.055935518453863776, 0...  0.604903   \n",
            "7  2023-01-03  [[0.018355014465572944, 0.018691514041332104, ...  0.578783   \n",
            "8  2023-01-03  [[0.004263860925549672, 0.006922971408802896, ...  0.508508   \n",
            "9  2023-01-03  [[0.047765557112874606, 0.05426056430681883, 0...  0.563300   \n",
            "10 2023-01-03  [[0.051547167208593536, 0.06157209873192007, 0...  0.552033   \n",
            "11 2023-01-03  [[0.033180583692361314, 0.035437199651026846, ...  0.593139   \n",
            "12 2023-01-03  [[0.03391649882343224, 0.034185974056011516, 0...  0.581920   \n",
            "13 2023-01-03  [[0.08358784784333002, 0.09660991824620162, 0....  0.584154   \n",
            "14 2023-01-03  [[0.01316269163096403, 0.017091027272499545, 0...  0.578343   \n",
            "15 2023-01-03  [[0.016086644141120127, 0.01894996229095674, 0...  0.558891   \n",
            "16 2023-01-03  [[0.007622384320120791, 0.009231956496725532, ...  0.567151   \n",
            "17 2023-01-03  [[0.005057690287244165, 0.005337638822017319, ...  0.591039   \n",
            "18 2023-01-03  [[0.04464095786414843, 0.05795048803840694, 0....  0.655798   \n",
            "19 2023-01-03  [[0.03289583084028597, 0.035652975764457855, 0...  0.610521   \n",
            "\n",
            "    ticker_id  sector_id  \n",
            "0           0          0  \n",
            "1          55          4  \n",
            "2          57          4  \n",
            "3          69          4  \n",
            "4          47          0  \n",
            "5          11          5  \n",
            "6          18          5  \n",
            "7          33          5  \n",
            "8          46          0  \n",
            "9          36          5  \n",
            "10         37          5  \n",
            "11         38          5  \n",
            "12         45          0  \n",
            "13         54          5  \n",
            "14         62          5  \n",
            "15         78          5  \n",
            "16         26          0  \n",
            "17         21          6  \n",
            "18          3          0  \n",
            "19         72          7  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform walk-forward validation using the start and end dates from your data\n",
        "walk_forward_validation(\n",
        "    data_df=combined_df,\n",
        "    sector_id=4,  # Consumer Cyclic Sector\n",
        "    params=params_consumer_cyclical,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date,\n",
        "    train_window_days=365,\n",
        "    test_window_days=30\n",
        ")"
      ],
      "metadata": {
        "id": "VoQfNtDB-Ent"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IU-qRLBZ-4oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- INDUSTRIALS -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "LCn2EL5S-Ehq"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Industrial sector\n",
        "industrial_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 5]\n",
        "\n",
        "# Get stock names correctly\n",
        "industrial_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(industrial_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Industrial sector:\")\n",
        "print(industrial_stock_names)\n"
      ],
      "metadata": {
        "id": "GXD8UUDU-Eac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d30030-6785-4485-b17d-406032291d4c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stocks being trained in Industrial sector:\n",
            "['EMR', 'FDX', 'GD', 'GE', 'BA', 'HON', 'RTX', 'CAT', 'LMT', 'MMM']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_industrials = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}"
      ],
      "metadata": {
        "id": "NawPCmimCE4E"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "WvxwP30g_Oxn"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "aXvGd9pL_OmL"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "Bpwv_YwT_Ofw"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "def walk_forward_validation(data_df, sector_id, params, start_date, end_date, train_window_days=365, test_window_days=30):\n",
        "    \"\"\"\n",
        "    Perform walk-forward validation on sector-specific data.\n",
        "\n",
        "    Parameters:\n",
        "        data_df: DataFrame with columns ['Date', 'X', 'y', 'ticker_id', 'sector_id']\n",
        "        sector_id: Sector ID (e.g., 0 for Technology)\n",
        "        params: Model parameters dictionary\n",
        "        start_date: Start of the walk-forward validation window (string or datetime)\n",
        "        end_date: End of the walk-forward validation window (string or datetime)\n",
        "        train_window_days: Size of training window in days\n",
        "        test_window_days: Size of testing window in days\n",
        "    \"\"\"\n",
        "    # Convert dates\n",
        "    start = pd.to_datetime(start_date)\n",
        "    end = pd.to_datetime(end_date)\n",
        "\n",
        "    current_date = start\n",
        "    walk_id = 0\n",
        "\n",
        "    while current_date + timedelta(days=train_window_days + test_window_days) <= end:\n",
        "        train_start = current_date\n",
        "        train_end = train_start + timedelta(days=train_window_days)\n",
        "        test_start = train_end\n",
        "        test_end = test_start + timedelta(days=test_window_days)\n",
        "\n",
        "        print(f\"\\n=== Walk {walk_id}: {train_start.date()} to {test_end.date()} ===\")\n",
        "\n",
        "        # Filter training data\n",
        "        train_data = data_df[(data_df['Date'] >= train_start) & (data_df['Date'] < train_end) & (data_df['sector_id'] == sector_id)]\n",
        "        test_data = data_df[(data_df['Date'] >= test_start) & (data_df['Date'] < test_end) & (data_df['sector_id'] == sector_id)]\n",
        "\n",
        "        if len(train_data) == 0 or len(test_data) == 0:\n",
        "            print(\"Skipping due to lack of data.\")\n",
        "            current_date += timedelta(days=test_window_days)\n",
        "            continue\n",
        "\n",
        "        # Convert to arrays\n",
        "        X_train = np.stack(train_data['X'].values)\n",
        "        y_train = np.stack(train_data['y'].values)\n",
        "        train_tickers = train_data['ticker_id'].values\n",
        "        train_sectors = train_data['sector_id'].values\n",
        "\n",
        "        X_test = np.stack(test_data['X'].values)\n",
        "        y_test = np.stack(test_data['y'].values)\n",
        "        test_tickers = test_data['ticker_id'].values\n",
        "\n",
        "        # Train model on current window\n",
        "        sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id, params)\n",
        "\n",
        "        # Predict\n",
        "        log_returns = predict_logreturns(sector_models[sector_id], X_test, test_tickers, sector_id, scalers)\n",
        "\n",
        "        # Volatility\n",
        "        volatility = calculate_volatility(log_returns)\n",
        "\n",
        "        # Save predictions\n",
        "        output_filename = f'Industrial_sector_predictions_walk{walk_id}.csv'\n",
        "        save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping)\n",
        "\n",
        "        # Advance window\n",
        "        current_date += timedelta(days=test_window_days)\n",
        "        walk_id += 1\n"
      ],
      "metadata": {
        "id": "0oPGmspw_OZX"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract start and end dates from your original dataset (df)\n",
        "start_date = df['Date'].min()\n",
        "end_date = df['Date'].max()\n",
        "\n",
        "# Create the combined DataFrame once lengths are confirmed to match\n",
        "combined_df = pd.DataFrame({\n",
        "    'Date': df['Date'].iloc[:len(X_train)],  # Make sure we slice df['Date'] to match X_train length\n",
        "    'X': list(X_train),                     # X_train: 2D sequences\n",
        "    'y': y_train.flatten(),                 # y_train: Flattened target values\n",
        "    'ticker_id': train_tickers,             # Assuming train_tickers holds the ticker IDs\n",
        "    'sector_id': train_sectors              # Assuming train_sectors holds the sector IDs\n",
        "})\n",
        "\n",
        "# Optional: Sort by date for chronological order\n",
        "combined_df = combined_df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Preview the combined_df\n",
        "print(combined_df.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5zVnCqZZ6_l",
        "outputId": "2507d321-52a8-4656-9c13-af4604da8e63"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date                                                  X         y  \\\n",
            "0  2023-01-03  [[0.021892605168306947, 0.030994682917296866, ...  0.614997   \n",
            "1  2023-01-03  [[0.03383293152758216, 0.04147293256184843, 0....  0.579069   \n",
            "2  2023-01-03  [[0.052016600173119154, 0.056808307454200284, ...  0.575439   \n",
            "3  2023-01-03  [[0.01859204511910545, 0.02327759158298365, 0....  0.577445   \n",
            "4  2023-01-03  [[0.11329244047522373, 0.13836609063012026, 0....  0.578943   \n",
            "5  2023-01-03  [[0.03375618540322741, 0.046241754179337714, 0...  0.638673   \n",
            "6  2023-01-03  [[0.05128331701461588, 0.055935518453863776, 0...  0.604903   \n",
            "7  2023-01-03  [[0.018355014465572944, 0.018691514041332104, ...  0.578783   \n",
            "8  2023-01-03  [[0.004263860925549672, 0.006922971408802896, ...  0.508508   \n",
            "9  2023-01-03  [[0.047765557112874606, 0.05426056430681883, 0...  0.563300   \n",
            "10 2023-01-03  [[0.051547167208593536, 0.06157209873192007, 0...  0.552033   \n",
            "11 2023-01-03  [[0.033180583692361314, 0.035437199651026846, ...  0.593139   \n",
            "12 2023-01-03  [[0.03391649882343224, 0.034185974056011516, 0...  0.581920   \n",
            "13 2023-01-03  [[0.08358784784333002, 0.09660991824620162, 0....  0.584154   \n",
            "14 2023-01-03  [[0.01316269163096403, 0.017091027272499545, 0...  0.578343   \n",
            "15 2023-01-03  [[0.016086644141120127, 0.01894996229095674, 0...  0.558891   \n",
            "16 2023-01-03  [[0.007622384320120791, 0.009231956496725532, ...  0.567151   \n",
            "17 2023-01-03  [[0.005057690287244165, 0.005337638822017319, ...  0.591039   \n",
            "18 2023-01-03  [[0.04464095786414843, 0.05795048803840694, 0....  0.655798   \n",
            "19 2023-01-03  [[0.03289583084028597, 0.035652975764457855, 0...  0.610521   \n",
            "\n",
            "    ticker_id  sector_id  \n",
            "0           0          0  \n",
            "1          55          4  \n",
            "2          57          4  \n",
            "3          69          4  \n",
            "4          47          0  \n",
            "5          11          5  \n",
            "6          18          5  \n",
            "7          33          5  \n",
            "8          46          0  \n",
            "9          36          5  \n",
            "10         37          5  \n",
            "11         38          5  \n",
            "12         45          0  \n",
            "13         54          5  \n",
            "14         62          5  \n",
            "15         78          5  \n",
            "16         26          0  \n",
            "17         21          6  \n",
            "18          3          0  \n",
            "19         72          7  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform walk-forward validation using the start and end dates from your data\n",
        "walk_forward_validation(\n",
        "    data_df=combined_df,\n",
        "    sector_id=5,  # Industrial Sector\n",
        "    params=params_industrials,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date,\n",
        "    train_window_days=365,\n",
        "    test_window_days=30\n",
        ")"
      ],
      "metadata": {
        "id": "_Ej3Z_uYZ68K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qn366WO6_OTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- COMMUNICATION SERVICES -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "bA8KmlxI_OM3"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Communication Services sector\n",
        "commserv_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 6]\n",
        "\n",
        "# Get stock names correctly\n",
        "commserv_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(commserv_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Communication Services sector:\")\n",
        "print(commserv_stock_names)\n"
      ],
      "metadata": {
        "id": "-LqaRVgM_OFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1380e6-26f0-4315-d8f6-5607a0b0adc1"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stocks being trained in Communication Services sector:\n",
            "['NFLX', 'GOOG', 'CHTR', 'CMCSA', 'META', 'DIS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_communication_servicess = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}"
      ],
      "metadata": {
        "id": "eE8ttvIZCEwg"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "Vb-2GlODAQjW"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "h3jXUJRSAPaR"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "lGgQMcb8APDp"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "def walk_forward_validation(data_df, sector_id, params, start_date, end_date, train_window_days=365, test_window_days=30):\n",
        "    \"\"\"\n",
        "    Perform walk-forward validation on sector-specific data.\n",
        "\n",
        "    Parameters:\n",
        "        data_df: DataFrame with columns ['Date', 'X', 'y', 'ticker_id', 'sector_id']\n",
        "        sector_id: Sector ID (e.g., 0 for Technology)\n",
        "        params: Model parameters dictionary\n",
        "        start_date: Start of the walk-forward validation window (string or datetime)\n",
        "        end_date: End of the walk-forward validation window (string or datetime)\n",
        "        train_window_days: Size of training window in days\n",
        "        test_window_days: Size of testing window in days\n",
        "    \"\"\"\n",
        "    # Convert dates\n",
        "    start = pd.to_datetime(start_date)\n",
        "    end = pd.to_datetime(end_date)\n",
        "\n",
        "    current_date = start\n",
        "    walk_id = 0\n",
        "\n",
        "    while current_date + timedelta(days=train_window_days + test_window_days) <= end:\n",
        "        train_start = current_date\n",
        "        train_end = train_start + timedelta(days=train_window_days)\n",
        "        test_start = train_end\n",
        "        test_end = test_start + timedelta(days=test_window_days)\n",
        "\n",
        "        print(f\"\\n=== Walk {walk_id}: {train_start.date()} to {test_end.date()} ===\")\n",
        "\n",
        "        # Filter training data\n",
        "        train_data = data_df[(data_df['Date'] >= train_start) & (data_df['Date'] < train_end) & (data_df['sector_id'] == sector_id)]\n",
        "        test_data = data_df[(data_df['Date'] >= test_start) & (data_df['Date'] < test_end) & (data_df['sector_id'] == sector_id)]\n",
        "\n",
        "        if len(train_data) == 0 or len(test_data) == 0:\n",
        "            print(\"Skipping due to lack of data.\")\n",
        "            current_date += timedelta(days=test_window_days)\n",
        "            continue\n",
        "\n",
        "        # Convert to arrays\n",
        "        X_train = np.stack(train_data['X'].values)\n",
        "        y_train = np.stack(train_data['y'].values)\n",
        "        train_tickers = train_data['ticker_id'].values\n",
        "        train_sectors = train_data['sector_id'].values\n",
        "\n",
        "        X_test = np.stack(test_data['X'].values)\n",
        "        y_test = np.stack(test_data['y'].values)\n",
        "        test_tickers = test_data['ticker_id'].values\n",
        "\n",
        "        # Train model on current window\n",
        "        sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id, params)\n",
        "\n",
        "        # Predict\n",
        "        log_returns = predict_logreturns(sector_models[sector_id], X_test, test_tickers, sector_id, scalers)\n",
        "\n",
        "        # Volatility\n",
        "        volatility = calculate_volatility(log_returns)\n",
        "\n",
        "        # Save predictions\n",
        "        output_filename = f'CommunicationServices_sector_predictions_walk{walk_id}.csv'\n",
        "        save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping)\n",
        "\n",
        "        # Advance window\n",
        "        current_date += timedelta(days=test_window_days)\n",
        "        walk_id += 1\n"
      ],
      "metadata": {
        "id": "e3AhDqLvAPAw"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract start and end dates from your original dataset (df)\n",
        "start_date = df['Date'].min()\n",
        "end_date = df['Date'].max()\n",
        "\n",
        "# Create the combined DataFrame once lengths are confirmed to match\n",
        "combined_df = pd.DataFrame({\n",
        "    'Date': df['Date'].iloc[:len(X_train)],  # Make sure we slice df['Date'] to match X_train length\n",
        "    'X': list(X_train),                     # X_train: 2D sequences\n",
        "    'y': y_train.flatten(),                 # y_train: Flattened target values\n",
        "    'ticker_id': train_tickers,             # Assuming train_tickers holds the ticker IDs\n",
        "    'sector_id': train_sectors              # Assuming train_sectors holds the sector IDs\n",
        "})\n",
        "\n",
        "# Optional: Sort by date for chronological order\n",
        "combined_df = combined_df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Preview the combined_df\n",
        "print(combined_df.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL_qtx1EaLzd",
        "outputId": "ef5be26e-d971-4423-e797-9c9d83be6bf4"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date                                                  X         y  \\\n",
            "0  2023-01-03  [[0.021892605168306947, 0.030994682917296866, ...  0.614997   \n",
            "1  2023-01-03  [[0.03383293152758216, 0.04147293256184843, 0....  0.579069   \n",
            "2  2023-01-03  [[0.052016600173119154, 0.056808307454200284, ...  0.575439   \n",
            "3  2023-01-03  [[0.01859204511910545, 0.02327759158298365, 0....  0.577445   \n",
            "4  2023-01-03  [[0.11329244047522373, 0.13836609063012026, 0....  0.578943   \n",
            "5  2023-01-03  [[0.03375618540322741, 0.046241754179337714, 0...  0.638673   \n",
            "6  2023-01-03  [[0.05128331701461588, 0.055935518453863776, 0...  0.604903   \n",
            "7  2023-01-03  [[0.018355014465572944, 0.018691514041332104, ...  0.578783   \n",
            "8  2023-01-03  [[0.004263860925549672, 0.006922971408802896, ...  0.508508   \n",
            "9  2023-01-03  [[0.047765557112874606, 0.05426056430681883, 0...  0.563300   \n",
            "10 2023-01-03  [[0.051547167208593536, 0.06157209873192007, 0...  0.552033   \n",
            "11 2023-01-03  [[0.033180583692361314, 0.035437199651026846, ...  0.593139   \n",
            "12 2023-01-03  [[0.03391649882343224, 0.034185974056011516, 0...  0.581920   \n",
            "13 2023-01-03  [[0.08358784784333002, 0.09660991824620162, 0....  0.584154   \n",
            "14 2023-01-03  [[0.01316269163096403, 0.017091027272499545, 0...  0.578343   \n",
            "15 2023-01-03  [[0.016086644141120127, 0.01894996229095674, 0...  0.558891   \n",
            "16 2023-01-03  [[0.007622384320120791, 0.009231956496725532, ...  0.567151   \n",
            "17 2023-01-03  [[0.005057690287244165, 0.005337638822017319, ...  0.591039   \n",
            "18 2023-01-03  [[0.04464095786414843, 0.05795048803840694, 0....  0.655798   \n",
            "19 2023-01-03  [[0.03289583084028597, 0.035652975764457855, 0...  0.610521   \n",
            "\n",
            "    ticker_id  sector_id  \n",
            "0           0          0  \n",
            "1          55          4  \n",
            "2          57          4  \n",
            "3          69          4  \n",
            "4          47          0  \n",
            "5          11          5  \n",
            "6          18          5  \n",
            "7          33          5  \n",
            "8          46          0  \n",
            "9          36          5  \n",
            "10         37          5  \n",
            "11         38          5  \n",
            "12         45          0  \n",
            "13         54          5  \n",
            "14         62          5  \n",
            "15         78          5  \n",
            "16         26          0  \n",
            "17         21          6  \n",
            "18          3          0  \n",
            "19         72          7  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform walk-forward validation using the start and end dates from your data\n",
        "walk_forward_validation(\n",
        "    data_df=combined_df,\n",
        "    sector_id=6,  # Communication Services Sector\n",
        "    params=params_communication_servicess,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date,\n",
        "    train_window_days=365,\n",
        "    test_window_days=30\n",
        ")"
      ],
      "metadata": {
        "id": "95H6gqlWaLwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAdEg5DzAO-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-x-x-x-x-x-x-x-x-x-x-x-x- CONSUMER DEFENCE -x-x-x-x-x-x-x-x-x-x-x-x-"
      ],
      "metadata": {
        "id": "nhY-fXmtAO1n"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}  # use ticker_to_id\n",
        "\n",
        "# Filter tickers belonging to Consumer Defence sector\n",
        "consdef_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 7]\n",
        "\n",
        "# Get stock names correctly\n",
        "consdef_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(consdef_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Consumer Defence sector:\")\n",
        "print(consdef_stock_names)\n"
      ],
      "metadata": {
        "id": "Mp7AUVzFAOuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53541644-c187-452f-d2a7-7038545dcad1"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stocks being trained in Consumer Defence sector:\n",
            "['PEP', 'PG', 'PM', 'KHC', 'KO', 'CL', 'COST', 'MDLZ', 'MO']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_consumer_defensive = {\n",
        "    'input_dim': X_train.shape[2],  # Number of features\n",
        "    'hidden_dim': 64,\n",
        "    'output_dim': 1,\n",
        "    'ticker_dim': len(ticker_mapping),  # Total unique tickers\n",
        "    'embedding_dim': 8,\n",
        "    'sector_dim': len(sector_mapping),  # Total unique sectors\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32\n",
        "}"
      ],
      "metadata": {
        "id": "vjIoGvocDHJH"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_logreturns(sector_model, X_test, test_tickers, sector_id, scalers):\n",
        "    \"\"\"\n",
        "    Predict log returns for all stocks in the given sector.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        x_seq = X_test[i]\n",
        "        ticker_id = test_tickers[i]\n",
        "\n",
        "        # Reinitialize hidden and cell states\n",
        "        h_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "        c_prev = np.zeros((sector_model.hidden_dim, 1))\n",
        "\n",
        "        # Get the prediction for this sequence\n",
        "        y_pred, _, _ = sector_model.forward(x_seq, ticker_id, sector_id, h_prev, c_prev)\n",
        "        predictions.append(y_pred.flatten()[0])\n",
        "\n",
        "    # Inverse scale the predictions (log returns)\n",
        "    min_target, max_target = scalers['target']\n",
        "    predictions = np.array(predictions) * (max_target - min_target) + min_target\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "JZgE5rsoBRLo"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_volatility(log_returns, window_size=10):\n",
        "    \"\"\"\n",
        "    Calculate volatility (standard deviation) of log returns for a given window size.\n",
        "    \"\"\"\n",
        "    volatility = []\n",
        "    for i in range(len(log_returns)):\n",
        "        start = max(0, i - window_size + 1)\n",
        "        window = log_returns[start:i+1]\n",
        "        volatility.append(np.std(window))  # Standard deviation as volatility\n",
        "    return np.array(volatility)\n"
      ],
      "metadata": {
        "id": "hvq4fVzaBRIK"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping):\n",
        "    \"\"\"\n",
        "    Save log returns and volatility to a CSV file for a specific sector with stock and sector names.\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "    # Map ticker_ids to actual ticker names using ticker_mapping\n",
        "    ticker_names = [id_to_ticker[ticker_id] for ticker_id in test_tickers]\n",
        "\n",
        "    # Map sector_id to sector name using sector_mapping\n",
        "    sector_name = sector_mapping.get(sector_id, \"Unknown Sector\")\n",
        "\n",
        "    # Create DataFrame to store predictions\n",
        "    df = pd.DataFrame({\n",
        "        'Stock': ticker_names,\n",
        "        'Log_Returns': log_returns,\n",
        "        'Volatility': volatility,\n",
        "        'Sector': [sector_name] * len(test_tickers)  # Add the sector name instead of sector_id\n",
        "    })\n",
        "\n",
        "    # Check if the file already exists to determine if headers should be written\n",
        "    file_exists = os.path.exists(output_filename)\n",
        "\n",
        "    # Append to the CSV file (with headers only if the file doesn't exist)\n",
        "    df.to_csv(output_filename, index=False, mode='a', header=not file_exists)  # Only write header if the file doesn't exist\n",
        "\n",
        "    print(f\"Results for sector '{sector_name}' saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "-O01ErQYBRA4"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "def walk_forward_validation(data_df, sector_id, params, start_date, end_date, train_window_days=365, test_window_days=30):\n",
        "    \"\"\"\n",
        "    Perform walk-forward validation on sector-specific data.\n",
        "\n",
        "    Parameters:\n",
        "        data_df: DataFrame with columns ['Date', 'X', 'y', 'ticker_id', 'sector_id']\n",
        "        sector_id: Sector ID (e.g., 0 for Technology)\n",
        "        params: Model parameters dictionary\n",
        "        start_date: Start of the walk-forward validation window (string or datetime)\n",
        "        end_date: End of the walk-forward validation window (string or datetime)\n",
        "        train_window_days: Size of training window in days\n",
        "        test_window_days: Size of testing window in days\n",
        "    \"\"\"\n",
        "    # Convert dates\n",
        "    start = pd.to_datetime(start_date)\n",
        "    end = pd.to_datetime(end_date)\n",
        "\n",
        "    current_date = start\n",
        "    walk_id = 0\n",
        "\n",
        "    while current_date + timedelta(days=train_window_days + test_window_days) <= end:\n",
        "        train_start = current_date\n",
        "        train_end = train_start + timedelta(days=train_window_days)\n",
        "        test_start = train_end\n",
        "        test_end = test_start + timedelta(days=test_window_days)\n",
        "\n",
        "        print(f\"\\n=== Walk {walk_id}: {train_start.date()} to {test_end.date()} ===\")\n",
        "\n",
        "        # Filter training data\n",
        "        train_data = data_df[(data_df['Date'] >= train_start) & (data_df['Date'] < train_end) & (data_df['sector_id'] == sector_id)]\n",
        "        test_data = data_df[(data_df['Date'] >= test_start) & (data_df['Date'] < test_end) & (data_df['sector_id'] == sector_id)]\n",
        "\n",
        "        if len(train_data) == 0 or len(test_data) == 0:\n",
        "            print(\"Skipping due to lack of data.\")\n",
        "            current_date += timedelta(days=test_window_days)\n",
        "            continue\n",
        "\n",
        "        # Convert to arrays\n",
        "        X_train = np.stack(train_data['X'].values)\n",
        "        y_train = np.stack(train_data['y'].values)\n",
        "        train_tickers = train_data['ticker_id'].values\n",
        "        train_sectors = train_data['sector_id'].values\n",
        "\n",
        "        X_test = np.stack(test_data['X'].values)\n",
        "        y_test = np.stack(test_data['y'].values)\n",
        "        test_tickers = test_data['ticker_id'].values\n",
        "\n",
        "        # Train model on current window\n",
        "        sector_models = train_lstm_sectorwise(X_train, y_train, train_tickers, train_sectors, sector_id, params)\n",
        "\n",
        "        # Predict\n",
        "        log_returns = predict_logreturns(sector_models[sector_id], X_test, test_tickers, sector_id, scalers)\n",
        "\n",
        "        # Volatility\n",
        "        volatility = calculate_volatility(log_returns)\n",
        "\n",
        "        # Save predictions\n",
        "        output_filename = f'ConsumerDefence_sector_predictions_walk{walk_id}.csv'\n",
        "        save_predictions_to_csv(sector_id, log_returns, volatility, test_tickers, output_filename, ticker_mapping, sector_mapping)\n",
        "\n",
        "        # Advance window\n",
        "        current_date += timedelta(days=test_window_days)\n",
        "        walk_id += 1\n"
      ],
      "metadata": {
        "id": "Xdz70TyOaXvU"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract start and end dates from your original dataset (df)\n",
        "start_date = df['Date'].min()\n",
        "end_date = df['Date'].max()\n",
        "\n",
        "# Create the combined DataFrame once lengths are confirmed to match\n",
        "combined_df = pd.DataFrame({\n",
        "    'Date': df['Date'].iloc[:len(X_train)],  # Make sure we slice df['Date'] to match X_train length\n",
        "    'X': list(X_train),                     # X_train: 2D sequences\n",
        "    'y': y_train.flatten(),                 # y_train: Flattened target values\n",
        "    'ticker_id': train_tickers,             # Assuming train_tickers holds the ticker IDs\n",
        "    'sector_id': train_sectors              # Assuming train_sectors holds the sector IDs\n",
        "})\n",
        "\n",
        "# Optional: Sort by date for chronological order\n",
        "combined_df = combined_df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Preview the combined_df\n",
        "print(combined_df.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9tNQXhjaXr-",
        "outputId": "209c0176-dbef-427b-c97e-4c7c9bab6123"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date                                                  X         y  \\\n",
            "0  2023-01-03  [[0.021892605168306947, 0.030994682917296866, ...  0.614997   \n",
            "1  2023-01-03  [[0.03383293152758216, 0.04147293256184843, 0....  0.579069   \n",
            "2  2023-01-03  [[0.052016600173119154, 0.056808307454200284, ...  0.575439   \n",
            "3  2023-01-03  [[0.01859204511910545, 0.02327759158298365, 0....  0.577445   \n",
            "4  2023-01-03  [[0.11329244047522373, 0.13836609063012026, 0....  0.578943   \n",
            "5  2023-01-03  [[0.03375618540322741, 0.046241754179337714, 0...  0.638673   \n",
            "6  2023-01-03  [[0.05128331701461588, 0.055935518453863776, 0...  0.604903   \n",
            "7  2023-01-03  [[0.018355014465572944, 0.018691514041332104, ...  0.578783   \n",
            "8  2023-01-03  [[0.004263860925549672, 0.006922971408802896, ...  0.508508   \n",
            "9  2023-01-03  [[0.047765557112874606, 0.05426056430681883, 0...  0.563300   \n",
            "10 2023-01-03  [[0.051547167208593536, 0.06157209873192007, 0...  0.552033   \n",
            "11 2023-01-03  [[0.033180583692361314, 0.035437199651026846, ...  0.593139   \n",
            "12 2023-01-03  [[0.03391649882343224, 0.034185974056011516, 0...  0.581920   \n",
            "13 2023-01-03  [[0.08358784784333002, 0.09660991824620162, 0....  0.584154   \n",
            "14 2023-01-03  [[0.01316269163096403, 0.017091027272499545, 0...  0.578343   \n",
            "15 2023-01-03  [[0.016086644141120127, 0.01894996229095674, 0...  0.558891   \n",
            "16 2023-01-03  [[0.007622384320120791, 0.009231956496725532, ...  0.567151   \n",
            "17 2023-01-03  [[0.005057690287244165, 0.005337638822017319, ...  0.591039   \n",
            "18 2023-01-03  [[0.04464095786414843, 0.05795048803840694, 0....  0.655798   \n",
            "19 2023-01-03  [[0.03289583084028597, 0.035652975764457855, 0...  0.610521   \n",
            "\n",
            "    ticker_id  sector_id  \n",
            "0           0          0  \n",
            "1          55          4  \n",
            "2          57          4  \n",
            "3          69          4  \n",
            "4          47          0  \n",
            "5          11          5  \n",
            "6          18          5  \n",
            "7          33          5  \n",
            "8          46          0  \n",
            "9          36          5  \n",
            "10         37          5  \n",
            "11         38          5  \n",
            "12         45          0  \n",
            "13         54          5  \n",
            "14         62          5  \n",
            "15         78          5  \n",
            "16         26          0  \n",
            "17         21          6  \n",
            "18          3          0  \n",
            "19         72          7  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform walk-forward validation using the start and end dates from your data\n",
        "walk_forward_validation(\n",
        "    data_df=combined_df,\n",
        "    sector_id=7,  # Consumer Defence Sector\n",
        "    params=params_consumer_defensive,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date,\n",
        "    train_window_days=365,\n",
        "    test_window_days=30\n",
        ")"
      ],
      "metadata": {
        "id": "KoDzfBopBQ6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rb56MJX6BQzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to check train-test split for each sector"
      ],
      "metadata": {
        "id": "M7YwmlmZafPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_stocks_in_sector(train_tickers, train_sectors, test_tickers, test_sectors, ticker_mapping, sector_id, sector_mapping):\n",
        "    \"\"\"\n",
        "    Prints the unique stocks in train and test data for a given sector.\n",
        "    \"\"\"\n",
        "    # Reverse mapping: ID -> Ticker Name\n",
        "    ticker_to_id = {v: k for k, v in ticker_mapping.items()}\n",
        "\n",
        "    # Get tickers for the sector in train and test\n",
        "    train_sector_tickers = [ticker for ticker, sector in zip(train_tickers, train_sectors) if sector == sector_id]\n",
        "    test_sector_tickers = [ticker for ticker, sector in zip(test_tickers, test_sectors) if sector == sector_id]\n",
        "\n",
        "    unique_train_tickers = set(train_sector_tickers)\n",
        "    unique_test_tickers = set(test_sector_tickers)\n",
        "\n",
        "    # Map ticker IDs back to names\n",
        "    train_stock_names = [id_to_ticker[ticker_id] for ticker_id in unique_train_tickers]\n",
        "    test_stock_names = [id_to_ticker[ticker_id] for ticker_id in unique_test_tickers]\n",
        "\n",
        "    sector_name = sector_mapping.get(sector_id, f\"Sector {sector_id}\")\n",
        "\n",
        "    print(f\"\\n--- {sector_name} ---\")\n",
        "    print(f\"Unique stocks in train data ({len(train_stock_names)} stocks): {sorted(train_stock_names)}\")\n",
        "    print(f\"Unique stocks in test data ({len(test_stock_names)} stocks): {sorted(test_stock_names)}\")\n"
      ],
      "metadata": {
        "id": "i5myD4SrafMJ"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping properly\n",
        "id_to_ticker = {v: k for k, v in ticker_to_id.items()}\n",
        "\n",
        "# Filter tickers belonging to Technology sector (sector_id = 0)\n",
        "technology_sector_tickers = [ticker_id for ticker_id, sector_id in zip(train_tickers, train_sectors) if sector_id == 0]\n",
        "\n",
        "# Get stock names correctly\n",
        "technology_stock_names = [id_to_ticker[int(ticker_id)] for ticker_id in set(technology_sector_tickers)]\n",
        "\n",
        "print(\"Stocks being trained in Technology sector:\")\n",
        "print(technology_stock_names)"
      ],
      "metadata": {
        "id": "cqdieB0JFMm3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b30bbd-87ce-4482-e8d8-98d18c5292dc"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stocks being trained in Technology sector:\n",
            "['AAPL', 'MSFT', 'ACN', 'ADBE', 'NVDA', 'ORCL', 'AVGO', 'IBM', 'INTC', 'INTU', 'QCOM', 'CRM', 'CSCO']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}